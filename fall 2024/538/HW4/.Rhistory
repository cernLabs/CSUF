# visualizations
plot.ts(obj1$alpha)
plot.ts(obj1$beta)
# visualizations
plot.ts(obj1$alpha)
plot.ts(obj1$beta)
# visualizations
plot.ts(obj1$alpha)
plot.ts(obj1$beta)
acf(obj1$alpha)
acf(obj1$beta)
# take out the
k = 40*(1:10000)
k = k[k <= 10000]
# take out the
k = 40*(1:10000)
k = k[k <= 10000]
# visualizations
plot.ts(obj1$alpha[k])
plot.ts(obj1$beta[k])
acf(obj1$alpha[k])
acf(obj1$beta[k])
for(d in 1:10){
plot(density(obj1[d,]))
}
plot(density(obj1$theta[d,]))
for(d in 1:10){
plot(density(obj1$theta[d,]))
}
obj1$theta
obj1$theta -> theta
View(theta)
# in class code with MH and gibbs sampling
# predef the beyes functions
Prior <- function(a,b){
(a + b)^(-5/2)
}
LLH <- function(theta,a,b){
sum(log(dbeta(theta,a,b)))
}
Proposal <- function(a,b){ #Jacobian
1/(a*b)
}
rProposal <- function(n,mean,cov){
rmvnorm(n,mean,cov)
}
# build a function just for this algorithm
MHGIBBs <- function(y,N,B,alpha0,beta0,S.tune = diag(2)){
# initializations
J = length(y)
accept = 0
alpha.post = beta.post = numeric()
theta.post = matrix(0,J,B)
theta0 <- numeric(length = J)
#loop
for(b in 1:B){
# Gibbs Step for theta
for(j in 1:J){
shp1 =  alpha0 + y[j]
shp2 = beta0 + N[j] - y[j]
theta0[j] = rbeta(1, shp1, shp2)
}
# Metro-Haste step for alpha and beta
phi1 = rProposal(1, c(log(alpha0),log(beta0)), 1*S.tune)
alpha1 =  exp(phi1[1])
beta1 = exp(phi1[2])
r = exp(
LLH(theta0,alpha1,beta1)
+ log(Prior(alpha1,beta1))
+ log(Proposal(alpha0,beta0))
- LLH(theta0,alpha0,beta0)
- log(Prior(alpha0,beta0))
- log(Proposal(alpha1,beta1)))
## accept check
if(runif(1) < min(1,r)){
alpha0 = alpha1
beta0 = beta1
accept = accept + 1
}
# drop off the samplings
alpha.post[b] <- alpha0
beta.post[b] <- beta0
theta.post[,b] <- theta0
}
# tuning the covariance matrix
S.tune <- matrix(0,2,2)
S.tune[2,1] <- S.tune[1,2] <- cov(log(alpha.post),log(beta.post))
S.tune[1,1] <- var(log(alpha.post))
S.tune[2,2] <- var(log(beta.post))
print(accept/B)
# attributes in the function
return(list("alpha" = alpha.post, "beta" = beta.post, "theta" = theta.post, "AR" = accept/B, "S" = S.tune))
}
# let's run our functions
y <- bike$Bicycles
N <- bike$Bicycles + bike$OtherVehicles
mean(y/N) # this is about .2, make alpha0 = 2, beta0 = 8
MHGIBBs(y,N,5000,2,8)$S -> S1 # run 1 time to get tuning matrix
MHGIBBs(y,N,10000,2,8,S1) -> obj1
# visualizations
plot.ts(obj1$alpha)
plot.ts(obj1$beta)
acf(obj1$alpha)
acf(obj1$beta)
# take out all but the 40th lag
k = 20*(1:10000)
k = k[k <= 10000]
# visualizations
plot.ts(obj1$alpha[k])
plot.ts(obj1$beta[k])
acf(obj1$alpha[k])
acf(obj1$beta[k])
for(d in 1:10){
plot(density(obj1$theta[d,]))
}
for(d in 1:10){
plot(density(obj1$theta[d,]))
abline(v = (y/N)[d])
}
for(d in 1:10){
plot(density(obj1$theta[d,]),main = "density of theta_j with raw proportion")
abline(v = (y/N)[d])
}
# take out all but the 20th lag
k = 10*(1:10000)
k = k[k <= 10000]
# visualizations
plot.ts(obj1$alpha[k])
plot.ts(obj1$beta[k])
acf(obj1$alpha[k])
acf(obj1$beta[k])
par(mfrow = c(2,2))
for(d in 1:10){
plot(density(obj1$theta[d,]),main = "density of theta_j with jth raw proportion")
abline(v = (y/N)[d])
}
par(mfrow = c(2,2))
for(d in 1:10){
plot(density(obj1$theta[d,]),main = paste("Density of theta_", d, "with raw proportion",d))
abline(v = (y/N)[d])
}
par(mfrow = c(2,2))
for(d in 1:10){
plot(density(obj1$theta[d,]),main = paste("Density of theta_",d, "with raw proportion",d))
abline(v = (y/N)[d])
}
# visualizations
par(mfrow = c(2,2))
plot.ts(obj1$alpha)
plot.ts(obj1$beta)
acf(obj1$alpha)
acf(obj1$beta)
# visualizations
par(mfrow = c(2,2))
plot.ts(obj1$alpha[k])
plot.ts(obj1$beta[k])
acf(obj1$alpha[k])
acf(obj1$beta[k])
par(mfrow = c(2,2))
for(d in 1:10){
plot(density(obj1$theta[d,k]),main = paste("Density of theta_",d, "with raw proportion",d))
abline(v = (y/N)[d])
}
plot(density(as.vector(obj1$theta[,k])))
plot(density(as.vector(obj1$theta[,k])))
quantile(obj1$theta[,k], c(0.025,.975))
# input alphas and betas into a beta distribution
rbeta(1000,obj1$alpha[k],obj1$beta[k])
# input alphas and betas into a beta distribution
rbeta(1000,obj1$alpha[k],obj1$beta[k]) -> newtheta
?rbinom
rbinom(1000,100,newtheta) -> newy
plot(density(newy))
plot(density(newy), main = "y_new")
quantile(newy,c(.025,.975))
# checking analytically if this makes sense
CI = matrix(0, ncol = 3, nrow = 10)
for(j in 1:10){
CI[j,] = quantile(obj1$theta[j,k], probs = c(0.025,0.5, 0.975))
}
plot(y/N, y/N, type = "l")
points(y/N, CI[,2], pch = 19, col = 3)
for(j in 1:10){
points(c(y[j]/N[j],y[j]/N[j]),  c(CI[j,1],CI[j,3]), type ="l", col = 4)
}
# setup the data
schools_data <- list(
J = 8,
y = c(28, 8, -3, 7, -1, 1, 18, 12),
sigma = c(15, 10, 16, 11, 9, 11, 10, 18)
)
# setup the data
schools <- list(
J = 8,
y = c(28, 8, -3, 7, -1, 1, 18, 12),
sigma = c(15, 10, 16, 11, 9, 11, 10, 18)
)
# run the STAN and fit the data
schools_fit <- stan(file="schools.stan",
data = schools,
iter = 1000,
chains = 4)
View(schools)
# run the STAN and fit the data
schools_fit <- stan(file="schools.stan",
data = c("J","y","sigma"),
iter = 1000,
chains = 4)
?stan()
# run the STAN and fit the data
schools_fit <- stan(file="schools.stan",
data = schools,
iter = 1000,
chains = 4)
# run the STAN and fit the data
schools_fit <- stan(file="schools.stan",
data = schools,
iter = 1000,
chains = 4)
# run the STAN and fit the data
schools_fit <- stan(file="schools.stan",
data = c("J","y","sigma"),
iter = 1000,
chains = 4)
J <- schools$J
y <- schools$y
sigma <- schools$sigma
# run the STAN and fit the data
schools_fit <- stan(file="schools.stan",
data = c("J","y","sigma"),
iter = 1000,
chains = 4)
# run the STAN and fit the data
schools_fit <- stan(file="schools.stan",
data = c("J","y","sigma"),
iter = 1000,
chains = 4)
# run the STAN and fit the data
schools_fit <- stan(file="schools.stan",
data = c("J","y","sigma"),
iter = 1000,
chains = 4)
# run the STAN and fit the data
schools_fit <- stan(file="schools.stan",
data = c("J","y","sigma"),
iter = 1000,
chains = 4)
# run the STAN and fit the data
schools_fit <- stan(file="schools.stan",
data = c("J","y","sigma"),
iter = 1000,
chains = 4)
print (schools_fit)
plot (schools_fit)
# run the STAN and fit the data
schools_fit <- stan(file="schools.stan",
data = schools,
iter = 1000,
chains = 4)
print (schools_fit)
plot (schools_fit)
# setup the data
schools <- read.csv(file = "schools.csv", header = T)
# run the STAN and fit the data
schools_fit <- stan(file="schools.stan",
data = schools,
iter = 1000,
chains = 4)
print (schools_fit)
plot (schools_fit)
# setup the data
schools <- read.csv(file = "schools.csv", header = T)
schools$J <- 8
# run the STAN and fit the data
schools_fit <- stan(file="schools.stan",
data = schools,
iter = 1000,
chains = 4)
print (schools_fit)
plot (schools_fit)
# setup the data
schools <- read.csv(file = "schools.csv", header = T)
schools <- list("J" = 8,
"y" = schools$estimate,
"sigma" = schools$sd)
# run the STAN and fit the data
schools_fit <- stan(file="schools.stan",
data = schools,
iter = 1000,
chains = 4)
print (schools_fit)
plot (schools_fit)
# run the STAN and fit the data
# schools_fit <- stan(file="schools.stan",
#                     data = schools,
#                     iter = 1000,
#                     chains = 4)
fit1 <- stan(
file = "schools.stan", # Stan program
data = schools,    # named list of data
chains = 4,             # number of Markov chains
warmup = 1000,          # number of warmup iterations per chain
iter = 20000,           # total number of iterations per chain
cores = 2,              # number of cores
refresh = 1000,         # show progress every 'refresh' iterations
thin = 10               # number of thinning
)
print (schools_fit)
plot (schools_fit)
View(fit1)
(fit1)
plot (fit1)
print (fit1)
plot (fit1)
# traceplot
traceplot(fit1, pars = c("mu", "tau"), inc_warmup = T, nrow = 2)
knitr::opts_chunk$set(echo = TRUE)
library(rstan)
library(mvtnorm)
rat <- read.csv("rat.csv", header = T)
plot(rat)
rat <- read.csv("rat.csv", header = T)
plot(rat, main = "", xlab = "number infected", ylab = "number of rats", )
rat <- read.csv("rat.csv", header = T)
plot(rat, main = "", xlab = "number infected", ylab = "number of rats")
rat <- read.csv("rat.csv", header = T)
plot(rat, xlab = "number infected", ylab = "number of rats")
rat <- read.csv("rat.csv", header = T)
plot(-rat, xlab = "number infected", ylab = "number of rats")
rat <- read.csv("rat.csv", header = T)
plot(rat$N, rat$y, xlab = "number infected", ylab = "number of rats")
hist(rat$y/rat$N)
hist(rat$y/rat$N,breaks = 71)
hist(rat$y/rat$N,breaks = 20)
rat <- read.csv("rat.csv", header = T)
plot(density(rat$y/rat$N))
rat <- read.csv("rat.csv", header = T)
plot(density(rat$y/rat$N))
?rnorm()
knitr::opts_chunk$set(echo = TRUE)
library(rstan)
library(mvtnorm)
# setting up data
bike <- read.csv("bike-data.csv", header = T)
# in class code with MH and gibbs sampling
# predef the bayes functions
Prior <- function(a,b){
(a + b)^(-5/2)
}
LLH <- function(theta,a,b){
sum(log(dbeta(theta,a,b)))
}
Proposal <- function(a,b){ #Jacobian
1/(a*b)
}
rProposal <- function(n,mean,cov){
rmvnorm(n,mean,cov)
}
# build a function just for this algorithm
MHGIBBs <- function(y,N,B,alpha0,beta0,S.tune = diag(2)){
# initializations
J = length(y)
accept = 0
alpha.post = beta.post = numeric()
theta.post = matrix(0,J,B)
theta0 <- numeric(length = J)
#loop
for(b in 1:B){
# Gibbs Step for theta
for(j in 1:J){
shp1 =  alpha0 + y[j]
shp2 = beta0 + N[j] - y[j]
theta0[j] = rbeta(1, shp1, shp2)
}
# Metro-Haste step for alpha and beta
phi1 = rProposal(1, c(log(alpha0),log(beta0)), 1*S.tune)
alpha1 =  exp(phi1[1])
beta1 = exp(phi1[2])
r = exp(
LLH(theta0,alpha1,beta1)
+ log(Prior(alpha1,beta1))
+ log(Proposal(alpha0,beta0))
- LLH(theta0,alpha0,beta0)
- log(Prior(alpha0,beta0))
- log(Proposal(alpha1,beta1)))
## accept check
if(runif(1) < min(1,r)){
alpha0 = alpha1
beta0 = beta1
accept = accept + 1
}
# drop off the samplings
alpha.post[b] <- alpha0
beta.post[b] <- beta0
theta.post[,b] <- theta0
}
# tuning the covariance matrix
S.tune <- matrix(0,2,2)
S.tune[2,1] <- S.tune[1,2] <- cov(log(alpha.post),log(beta.post))
S.tune[1,1] <- var(log(alpha.post))
S.tune[2,2] <- var(log(beta.post))
print(accept/B)
# attributes in the function
return(list("alpha" = alpha.post, "beta" = beta.post, "theta" = theta.post, "AR" = accept/B, "S" = S.tune))
}
# let's run our functions
y <- bike$Bicycles
N <- bike$Bicycles + bike$OtherVehicles
mean(y/N) # this is about .2, make alpha0 = 2, beta0 = 8
MHGIBBs(y,N,5000,2,8)$S -> S1 # run 1 time to get tuning matrix
MHGIBBs(y,N,10000,2,8,S1) -> obj1
# visualizations
par(mfrow = c(2,2))
plot.ts(obj1$alpha)
plot.ts(obj1$beta)
acf(obj1$alpha)
acf(obj1$beta)
# take out all but the 10th lag
k = 10*(1:10000)
k = k[k <= 10000]
# visualizations
par(mfrow = c(2,2))
plot.ts(obj1$alpha[k])
plot.ts(obj1$beta[k])
acf(obj1$alpha[k])
acf(obj1$beta[k])
par(mfrow = c(2,2))
for(d in 1:10){
plot(density(obj1$theta[d,k]),main = paste("Density of theta_",d, "with raw proportion",d))
abline(v = (y/N)[d])
}
plot(density(as.vector(obj1$theta[,k])))
quantile(obj1$theta[,k], c(0.025,.975))
# input alphas and betas into a beta distribution
rbeta(1000,obj1$alpha[k],obj1$beta[k]) -> newtheta
rbinom(1000,100,newtheta) -> newy
plot(density(newy), main = "y_new")
quantile(newy,c(.025,.975))
# checking analytically if this makes sense
CI = matrix(0, ncol = 3, nrow = 10)
for(j in 1:10){
CI[j,] = quantile(obj1$theta[j,k], probs = c(0.025,0.5, 0.975))
}
plot(y/N, y/N, type = "l")
points(y/N, CI[,2], pch = 19, col = 3)
for(j in 1:10){
points(c(y[j]/N[j],y[j]/N[j]),  c(CI[j,1],CI[j,3]), type ="l", col = 4)
}
# setup the data
schools <- read.csv(file = "schools.csv", header = T)
schools <- list("J" = 8,
"y" = schools$estimate,
"sigma" = schools$sd)
# run the STAN and fit the data
# schools_fit <- stan(file="schools.stan",
#                     data = schools,
#                     iter = 1000,
#                     chains = 4)
fit1 <- stan(
file = "schools.stan", # Stan program
data = schools,    # named list of data
chains = 4,             # number of Markov chains
warmup = 1000,          # number of warmup iterations per chain
iter = 20000,           # total number of iterations per chain
cores = 2,              # number of cores
refresh = 1000,         # show progress every 'refresh' iterations
thin = 10               # number of thinning
)
print (fit1)
plot (fit1)
# checking analytically if this makes sense
CI = matrix(0, ncol = 3, nrow = 10)
for(j in 1:10){
CI[j,] = quantile(obj1$theta[j,k], probs = c(0.025,0.5, 0.975))
}
plot(y/N, y/N, type = "l")
abline(h = 0)
points(y/N, CI[,2], pch = 19, col = 3)
for(j in 1:10){
points(c(y[j]/N[j],y[j]/N[j]),  c(CI[j,1],CI[j,3]), type ="l", col = 4)
}
plot(density(newy), main = "y_new")
abline(v=c(3,54))
quantile(newy,c(.025,.975))
plot(density(newy), main = "y_new")
quantile(newy,c(.025,.975)) -> q1
abline(v=q1, lty = 2)
q1
alp <- obj1$alpha[k]
bet <- obj1$beta[k]
expec <- alp(bet + alp)
expec <- alp/(bet + alp)
alp <- obj1$alpha[k]
bet <- obj1$beta[k]
expec <- alp/(bet + alp)
plot(density(expec))
quantile(expec, c(0.025,.975))
alp <- obj1$alpha[k]
bet <- obj1$beta[k]
expec <- alp/(bet + alp)
plot(density(expec), main = "E(theta) = alpha / (beta + alpha)")
quantile(expec, c(0.025,.975))
alp <- obj1$alpha[k]
bet <- obj1$beta[k]
expec <- alp/(bet + alp)
plot(density(expec), main = "E(theta) = alpha / (beta + alpha) density")
quantile(expec, c(0.025,.975))
alp <- obj1$alpha[k]
bet <- obj1$beta[k]
expec <- alp/(bet + alp)
plot(density(expec), main = "density for E[theta] = alpha / (beta + alpha)")
quantile(expec, c(0.025,.975))
