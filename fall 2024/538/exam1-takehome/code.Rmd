---
title: "takehome"
output: pdf_document
date: "2024-11-02"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(EnvStats)
library(ggplot2)
```

## Question 1

```{r}
data <- read.csv("income20train-2.csv", header = T)
data[,2] -> y
```

#### (a)

```{r}
plot(data$X,data$x)
boxplot(y)
hist(y, breaks = 100)
summary(y)
mean(y)
```

Important to note that the incomes tend to be dense towards the minimum rather than being in the middle; these numbers skew heavily to the right. We can try to fit a Pareto distribution to this later to see how is models the data and run our test set against it. We have a median of 320084 and a mean of 341938.1 while he have a max of 1484705. Also note the amount of outliers in the boxplot and where these outliers are. Also note how these outliers range wider than that of the not unusual data. This shows support to the phenomenon of American income inequality (even among the 20% richest in the nation). 


#### (b)

* consider an initial $\alpha_{b-1}$
* sample $\beta_{b-1}$ from $Mono(n\alpha_{b-1} + 1, min(y_1,...,y_n))$
* enter loop for $B$ amount of times
  + sample $\alpha_{b}$ from $\Gamma(n+1, \sum^n_{i=1}[ln(y_i)] - nln(\beta_{b-1}))$
  + sample $\beta_{b}$ from $Mono(n\alpha_{b} + 1, min(y_1,...,y_n))$
  + $\beta_{b}$ is set to $\beta_{b-1}$
* these $\alpha$'s and $\beta$'s are stored in a vector.

#### (c)

```{r}
# prep the mono sample function 
rmono = function(n,alpha,beta){
  u = runif(n)
  x = exp(log(beta) + (log(u)/alpha))
  return(x)
}
# make this the last this 
GIBBS1 <- function(B,data,alpha.init){
  set.seed(90909)
  # define n
  n = length(data)
  # make vector spaces
  alpha <- beta <- rep(0,B)
  # apply initials
  alpha[1] = alpha.init
  beta[1] = rmono(1,n*alpha.init + 1, min(data))
  
  #loop
  for(b in 2:B){
    rgamma(1,n+1,sum(log(data)) - n*log(beta[b-1])) -> alpha[b]
    rmono(1,n*alpha[b] + 1, min(data)) -> beta[b]
  }
  # output
  theta = cbind(alpha,beta)
  return(theta)
}
GIBBS1(2000,y,1) -> theta
```

```{r}
par(mfrow = c(1,2))
plot.ts(theta[,1], main = "alpha posterior trace plot", ylab = "alpha")
plot.ts(theta[,2], main = "beta posterior trace plot", ylab = "beta")
acf(theta[,1], main = "alpha Autocorrelation")
acf(theta[,2], main = "beta Autocorrelation")
```
There seems to be clear stabilization with $\beta$ and $\alpha$ doesn't need much to be burnt off. I will remove the the first 49. 

```{r}
# burning
B = length(theta[,1])
theta.burn = theta[50:B,]
# visuals
par(mfrow = c(1,2))
plot.ts(theta.burn[,1], main = "alpha posterior burned trace plot", ylab = "alpha")
plot.ts(theta.burn[,2], main = "beta posterior burned trace plot", ylab = "beta")
acf(theta.burn[,1], main = "alpha burned")
acf(theta.burn[,2], main = "beta burned")
```

```{r}
# generate bivariate scatterplot
post <- data.frame(theta.burn)
ggplot(post, aes(x = alpha, y = beta)) + geom_point() + geom_density2d()
```

#### (d)

```{r}
# plot alpha|data
plot(density(theta.burn[,1]),
     main = "P(alpha|data)")
dens = density(theta.burn[,1])
point.est.a = dens$x[dens$y == max(dens$y)]
abline(v = point.est.a,lty=2)
# confidence interval
c(quantile(theta.burn[,1],.025),quantile(theta.burn[,1],.975))
# plot beta|data
plot(density(theta.burn[,2]),
     main = "P(beta|data)")
dens = density(theta.burn[,2])
point.est.b = dens$x[dens$y == max(dens$y)]
abline(v = point.est.b,lty=2)
# confidence interval
c(quantile(theta.burn[,2],.025),quantile(theta.burn[,2],.975))
```

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
param. & CI lower & mode & CI upper \\ \hline
$\alpha$ & 3.420024 & 3.50072 & 3.580153 \\ \hline
$\beta$ & 249994.6 & 250031.6 & 250033.7 
\end{tabular}
\end{table}

Note that the $\beta$ parameter can be interpreted as an average lower end of the income data. Our most dense point for this parameter is 250031.6 and this is very close to our data's actual minimum of 250034. 
On the note about the <more>

#### (d)

```{r}
set.seed(90909)
# get the the 2000 samples
ynew = rpareto(1951,location = theta.burn[,2], shape = theta.burn[,1])
ynew = sample(ynew, size = 2000, rep = T)
# visuals
dens = density(ynew)
plot(dens, main = "P(Ynew|Y_i)")
point.est.y = dens$x[dens$y ==max(dens$y)]
abline(v=point.est.y, lty =2)
# confidence interval
lowr = quantile(ynew,.025)
uppr = quantile(ynew,.975)
sprintf("Mode : %0.2f",point.est.y)
sprintf("Conf. Interval : ( %0.2f , %0.2f )",lowr,uppr)
```

```{r}
# test data
y.test <- as.vector(read.csv("income20test-2.csv", header = T)[,2])
# count how much falls into the confidence interval
I = as.numeric(y.test > lowr & y.test < uppr)
sum(I)/length(I)
```

The Pareto distribution seems to be an accurate tool for modeling the top 20% of the incomes of US adults. Our confidence interval captures more than 95% of the data as we see in the above script. We can also overly the predictive model over or actual test data and observe the similarities in the test data and predicted data; they almost follow each other exactly. The modes are almost similar too; they are presented below. At this current seed, they present only difference of 15.3 dollars which is arguably negligible considering these high end salaries.

```{r}
# overlay prediction on test data
dens.test = density(y.test)
plot(dens.test,
     ylim = c(0,8.5e-06),
     main = "Test Data and Predicted Data")
lines(dens,lty = 2, col = 'red')
legend("topright", legend = c("Test Data", "Predicted Data"), col = c("black", "red"), lty = c(1, 2))
# modes
point.est.y
dens.test$x[dens.test$y == max(dens.test$y)]
```

## Problem 2

```{r}
# get the rat data
rat = as.matrix(read.csv("rat-3.csv"))
```

#### problem (a)

```{r}
# EDA
# boxplot
adlib  = rat[rat[,2] == 0,1]
restr = rat[rat[,2] == 1,1]
boxplot(adlib,restr,names = c("Ad libitum", "Restricted"), horizontal = T)
# 5p summaries
summary(restr)
summary(adlib)
```


```{r}
plot(density(restr), ylim = c(0,1.75), col = "blue", main = "Density of Rat Expectancies")
lines(density(adlib),col = 'red')
legend("topright", legend = c("Restticted", "Ad libitum"), col = c("blue", "red"), lty = c(1,1))
```

We can observe that the the *Ad libitum* group tends to have a lower average expectancy that the Restricted group. There can support the speculation that rats eat themselves to death.
