a <- eigen(A)
sqm <- a$vectors %*% diag(sqrt(a$values)) %*% t(a$vectors)
sqm <- (sqm+t(sqm))/2
}
#function for generating data
gen <- function(n,p,mu,sigma,seed){
#generate data from a p-variate normal with mean mu and covaraince sigma
#set seed to 2024
set.seed(seed)
#generate data from normal
z <- matrix(rnorm(n*p),n,p)
datan <- z %*% sqrtm(sigma) + matrix(mu,n,p,byrow = TRUE)
datan
}
# putting in the data
sig <- matrix(c(1,0.7,0.7,0.7,1,0.7,0.7,0.7,1), nrow = 3, ncol = 3)
mu <- matrix(c(-1,1,2), nrow =3)
x <- gen(200,3,mu,sig,2025)
I <- diag(3)
mu_0 <- matrix(0,3,1)
# initials
I <- diag(3)
mu_0 <- matrix(0,3,1)
abstol = 1e-05
# turn theta into a mu and sigma
from.theta <- function(p,theta){
mu <- theta[1:p]
sig <- matrix(0, nrow = p, ncol = p)
k = p + 1
for (i in 1:p){
for (j in 1:i){
sig[i,j] <- theta[k]
sig[j,i] <- sig[i,j]
k = k + 1
}
}
list(mu = mu, sig = sig)
}
# # compile Sigma and Mu into a single theta vector
to.theta <- function(mu,sig){
p <- nrow(sig)
theta <- matrix(0,nrow = p + p*(1+p)/2,ncol = 1)
theta[1:p] <- mu
k = p + 1
for(i in 1:p){
for(j in 1:i){
theta[k] <- sig[i,j]
k = k + 1
}
}
return(theta)
}
# make gradient
gradient <- function(x,mu,sig){
p <- nrow(sig)
n <- nrow(x)
inv.sig <- solve(sig)
# set initials
xi.sum <- matrix(0, p, 1)
C.mu <- matrix(0, p, p)
# compute sum of Xi and sum C(mu)
for(i in 1:n){
xi <- x[i,] - mu
xi.sum <- xi.sum + xi
C.mu <- C.mu + xi %*% t(xi)
}
# place elements into gradient mu and gradient sig
grad.mu <- inv.sig %*% xi.sum
A <- (n * inv.sig) - inv.sig %*% C.mu %*% inv.sig
grad.sig <- matrix(0, nrow = nrow(A), ncol = ncol(A))
#gradient sig
for(i in 1:nrow(sig)){
grad.sig[i,i] <- -(1/2) * A[i,i]
}
for(i in 1:nrow(sig)-1){
for (j in (i+1):ncol(sig)){
grad.sig[i,j] <- -1 * A[i,j]
grad.sig[j,i] <- grad.sig[i,j]
}
}
grad.norm <- norm(to.theta(grad.mu,grad.sig), type = '2')
list(grad.mu = grad.mu, grad.sig = grad.sig, grad.norm = grad.norm)
}
#likelihood function
likemvn <- function (x,mu,sig) {
# computes the likelihood and the gradient for multivariate normal
n = nrow(x)
p = ncol(x)
sig.inv <- solve(sig)
C.mu = matrix(0,p,p) # initializing sum of (xi-mu)(xi-mu)^T
xi.sum = matrix(0,p,1) # initializing sum of xi-mu
for (i in 1:n){
xi = x[i,] - mu
C.mu = C.mu + xi %*% t(xi)
}
ell = -(n*p*log(2*pi)+n*log(det(sig)) + sum(sig.inv * C.mu ))/2
return(ell)
}
# new function to run the optim() function
# Likelihood Function the passes theta vector
theta_opt <- function(theta,data){
x <- data
sig <- from.theta(x,theta)$sig
mu <- from.theta(x,theta)$mu
if(all(eigen(sig)$values>0)){
L <- likemvn(x,mu,sig)
} else {
L = NaN
}
return(L)
}
# gradient theta vector
grad_vec_opt <- function(theta,data){
x <- data
sig <- from.theta(x,theta)$sig
mu <- from.theta(x,theta)$mu
grad_sig <- gradient(x,mu,sig)$grad.sig
grad_mu <- gradient(x,mu,sig)$grad.mu
grad_theta <- to.theta(grad_mu,grad_sig)
return(grad_theta)
}
theta_0 <- to.theta(mu_0,I)
# running optim()
theta_0 <- to.theta(mu_0,I)
optim(par = theta_0,
fn = theta_opt,
gr = grad_vec_opt,
data = x,
method = "BFGS",
control = list(fnscale = -1, trace = 1, abstol = 1e-5),
hessian = TRUE)
# new function to run the optim() function
# Likelihood Function the passes theta vector
theta_opt <- function(theta,data){
x <- data
p <- ncol(x)
sig <- from.theta(p,theta)$sig
mu <- from.theta(p,theta)$mu
if(all(eigen(sig)$values>0)){
L <- likemvn(x,mu,sig)
} else {
L = NaN
}
return(L)
}
# gradient theta vector
grad_vec_opt <- function(theta,data){
x <- data
p <- ncol(x)
sig <- from.theta(p,theta)$sig
mu <- from.theta(p,theta)$mu
grad_sig <- gradient(x,mu,sig)$grad.sig
grad_mu <- gradient(x,mu,sig)$grad.mu
grad_theta <- to.theta(grad_mu,grad_sig)
return(grad_theta)
}
# running optim()
theta_0 <- to.theta(mu_0,I)
optim(par = theta_0,
fn = theta_opt,
gr = grad_vec_opt,
data = x,
method = "BFGS",
control = list(fnscale = -1, trace = 1, abstol = 1e-5),
hessian = TRUE)
setwd("~/Desktop/githubbahubba/CSUF/spring 2024/MATH 534/HW3b")
t(c(0,0,0))
th <- matrix(c(1,2,3),ncol=1)
th
th[1]
th[2]
th[3]
#building the likelihood
likelihood_wei <- function(t,d,w,a,b0,b1){
length(t) -> n
sum=0
for(i in 1:n){
sum = sum + (w[i]*log(a)+w[i]*(a-1)*log(t[i])-(t[i]Ë†(a))*exp(b0+d[i]*b1))
#building the likelihood
likelihood_wei <- function(t,d,w,a,b0,b1){
length(t) -> n
sum=0
for(i in 1:n){
sum = sum + (w[i]*log(a)+w[i]*(a-1)*log(t[i])-(t[i]^(a))*exp(b0+d[i]*b1))
}
return(sum)
}
#vectorize
to_theta <- function(a,b0,b1){
th <- matrix(c(a,b0,b1),ncol=1)
return(th)
}
# building the gradient function
gradient_wei <- function(t,d,w,a,b0,b1){
dLda = 0 # intials
dLdb0 = 0
dLdb1 = 0
length(t) -> n
for(i in 1:n){
dLda <- dLda + (w[i]/a+w[i]*log(t[i])-(t[i]^(a))*log(t[i])*exp(b0+d[i]*b1))
}
for(i in 1:n){
dLdb0 = dLdb0 - (t[i]^(a))*exp(b0+d[i]*b1)
}
for(i in 1:n){
dLdb1 = dLdb1 - (t[i]^(a))*exp(b0+d[i]*b1)*d[i]
}
vec <- matrix(c(dLda,dLdb0,dLdb1), nrow = 3)
return(vec)
}
# rendering the hessian
hessian_wei <- function(t,d,w,a,b0,b1){
H <- matrix(0,3,3)
length(t) -> n
#L_aa
for(i in 1:n){
H[1,1] = H[1,1] - w[i]/(a^2)-2*(t[i]^(a))*log(t[i])*exp(b0+d[i]*b1)
}
#L_ab0
for(i in 1:n){
H[2,1] = H[2,1] - (t[i]^(a))*log(t[i])*exp(b0+d[i]*b1)
}
H[1,2] = H[2,1]
#L_ab1
for(i in 1:n){
H[3,1] = H[3,1] - (t[i]^(a))*log(t[i])*exp(b0+d[i]*b1)*d[i]
}
H[1,3] = H[3,1]
#L_b0b0
for(i in 1:n){
H[2,2] = H[2,2] - (t[i]^(a))*exp(b0+d[i]*b1)
}
#L_b0b1
for(i in 1:n){
H[3,2] = H[3,2] - (t[i]^(a))*exp(b0+d[i]*b1)*d[i]
}
H[2,3] = H[3,2]
#L_b1b1
for(i in 1:n){
H[3,3] = H[3,3] - (t[i]^(a))*exp(b0+d[i]*b1)*d[i]^2
}
return(H)
}
# input data
t <- c(6,9,10,11,17,19,20,25,32,32,34,35,6,6,6,7,10,13,16,22,23,1,1,2,2,3,4,4,5,5,8,8,8,8,11,11,12,12,15,17,22,23)
d <- c(0,0,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,1,1,1,1,1 ,1 ,1 ,1 ,1 ,1,1,1,1,1,1,1,1,1,1,1,1,1,1 ,1 ,1 ,1 ,1,1,1,1)
w <- c(1,1,1 ,1 ,1 ,1 ,1 ,1 ,1 ,1 ,1 ,1 ,1,1,1,1,1 ,1 ,1 ,1 ,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
likelihood_wei(t,d,w,1,1,1)
gradient_wei(t,d,w,1,1,1)
hessian_wei(t,d,w,1,1,1)
# newtons method
newton2 <- function(t,d,w,a,b0,b1,maxit,tolerr,tolgrad){
header = paste0("Iteration", "      halving", "     log-likelihood","      ||Gradient||")
print(header)
it = 1
stop = FALSE
while(it <= maxit & stop == FALSE){
# first steps
theta0 <- to_theta(a,b0,b1)
L0 <- likelihood_wei(t,d,w,a,b0,b1)
#grad elements
grad_0 <- gradient_wei(t,d,w,a,b0,b1)
grad_norm <- norm(grad_0)
grad_a <- grad_0[1]
grad_b0 <- grad_0[2]
grad_b1 <- grad_0[3]
#get direction
hess <- hessian_wei(t,d,w,a,b0,b1)
inv_h <- solve(hess)
direc <- (-1)*(inv_h %*% grad_0)
#print
print(sprintf('%2.0f                 --          %3.4f               %.1e',
it,  L0, grad.on.norm))
#get new params
theta1 = theta0 + direc
a_n <- theta1[1]
b0_n <- theta1[2]
b1_n <- theta1[3]
grad_norm1 <- gradient_wei(t,d,w,a_n,b0_n,b1_n)
if(theta1[1] > 0){
L1 <- likelihood_wei(t,d,w,a_n,b0_n,b1_n)
} else {L1 <- NaN}
halve <- 0
if(it == 1 | it ==2 | it == 499 | it == 500){
print(sprintf('%2.0f                 %2.0f          %3.4f               %.1e',it,  halve,L1, grad_norm1))
}
while(have <= 20 & (theta1[1] <= 0 || L1 < L0)){
theta1 = theta0 + direc/(2^halve)
if(theta1[1] > 0){
a_n <- theta1[1]
b0_n <- theta1[2]
b1_n <- theta1[3]
L1 <- likelihood_wei(t,d,w,a_n,b0_n,b1_n)
grad_norm1 <- norm(gradient_wei(t,d,w,a_n,b0_n,b1_n))
}
print(sprintf('%2.0f                 %2.0f          %3.4f               %.1e',
it,  halve, L1, grad_norm1))
halve = halve + 1
if(it == 1 | it ==2 | it == 499 | it == 500){
print(sprintf('%2.0f                 %2.0f          %3.4f               %.1e',it,  halve,L1, grad_norm1))
}
if(it == 1 | it == 2 | it == 499){
print("-----------------------------------------------------------------")
print(header)
}
r.e = max(abs(theta0 - theta1)/abs(pmax(1,abs(theta0))))
if (r.e < tolerr & grad_norm1 < tolgrad){stop == TRUE}
a <- a_n
b0 <- b0_n
b1 <- b1_n
it <- it + 1
}
return(list("estimator of alpha"=a, "estimator of beta_0" = b0, "estimator of beta_1" = b1, "iteration" = it))
}
# newtons method
newton2 <- function(t,d,w,a,b0,b1,maxit,tolerr,tolgrad){
header = paste0("Iteration", "      halving", "     log-likelihood","      ||Gradient||")
print(header)
it = 1
stop = FALSE
while(it <= maxit & stop == FALSE){
# first steps
theta0 <- to_theta(a,b0,b1)
L0 <- likelihood_wei(t,d,w,a,b0,b1)
#grad elements
grad_0 <- gradient_wei(t,d,w,a,b0,b1)
grad_norm <- norm(grad_0)
grad_a <- grad_0[1]
grad_b0 <- grad_0[2]
grad_b1 <- grad_0[3]
#get direction
hess <- hessian_wei(t,d,w,a,b0,b1)
inv_h <- solve(hess)
direc <- (-1)*(inv_h %*% grad_0)
#print
print(sprintf('%2.0f                 --          %3.4f               %.1e',
it,  L0, grad.on.norm))
#get new params
theta1 = theta0 + direc
a_n <- theta1[1]
b0_n <- theta1[2]
b1_n <- theta1[3]
grad_norm1 <- gradient_wei(t,d,w,a_n,b0_n,b1_n)
if(theta1[1] > 0){
L1 <- likelihood_wei(t,d,w,a_n,b0_n,b1_n)
} else {L1 <- NaN}
halve <- 0
if(it == 1 | it ==2 | it == 499 | it == 500){
print(sprintf('%2.0f                 %2.0f          %3.4f               %.1e',it,  halve,L1, grad_norm1))
}
while(have <= 20 & (theta1[1] <= 0 || L1 < L0)){
theta1 = theta0 + direc/(2^halve)
if(theta1[1] > 0){
a_n <- theta1[1]
b0_n <- theta1[2]
b1_n <- theta1[3]
L1 <- likelihood_wei(t,d,w,a_n,b0_n,b1_n)
grad_norm1 <- norm(gradient_wei(t,d,w,a_n,b0_n,b1_n))
}
print(sprintf('%2.0f                 %2.0f          %3.4f               %.1e',
it,  halve, L1, grad_norm1))
halve = halve + 1
if(it == 1 | it ==2 | it == 499 | it == 500){
print(sprintf('%2.0f                 %2.0f          %3.4f               %.1e',it,  halve,L1, grad_norm1))
}
}
if(it == 1 | it == 2 | it == 499){
print("-----------------------------------------------------------------")
print(header)
}
r.e = max(abs(theta0 - theta1)/abs(pmax(1,abs(theta0))))
if (r.e < tolerr & grad_norm1 < tolgrad){stop == TRUE}
a <- a_n
b0 <- b0_n
b1 <- b1_n
it <- it + 1
}
return(list("estimator of alpha"=a, "estimator of beta_0" = b0, "estimator of beta_1" = b1, "iteration" = it))
}
newton2(t,d,w,1,1,1,500,1e-07,1e-07)
# newtons method
newton2 <- function(t,d,w,a,b0,b1,maxit,tolerr,tolgrad){
header = paste0("Iteration", "      halving", "     log-likelihood","      ||Gradient||")
print(header)
it = 1
stop = FALSE
while(it <= maxit & stop == FALSE){
# first steps
theta0 <- to_theta(a,b0,b1)
L0 <- likelihood_wei(t,d,w,a,b0,b1)
#grad elements
grad_0 <- gradient_wei(t,d,w,a,b0,b1)
grad_norm <- norm(grad_0)
grad_a <- grad_0[1]
grad_b0 <- grad_0[2]
grad_b1 <- grad_0[3]
#get direction
hess <- hessian_wei(t,d,w,a,b0,b1)
inv_h <- solve(hess)
direc <- (-1)*(inv_h %*% grad_0)
#print
print(sprintf('%2.0f                 --          %3.4f               %.1e',
it,  L0, grad_norm))
#get new params
theta1 = theta0 + direc
a_n <- theta1[1]
b0_n <- theta1[2]
b1_n <- theta1[3]
grad_norm1 <- gradient_wei(t,d,w,a_n,b0_n,b1_n)
if(theta1[1] > 0){
L1 <- likelihood_wei(t,d,w,a_n,b0_n,b1_n)
} else {L1 <- NaN}
halve <- 0
if(it == 1 | it ==2 | it == 499 | it == 500){
print(sprintf('%2.0f                 %2.0f          %3.4f               %.1e',it,  halve,L1, grad_norm1))
}
while(have <= 20 & (theta1[1] <= 0 || L1 < L0)){
theta1 = theta0 + direc/(2^halve)
if(theta1[1] > 0){
a_n <- theta1[1]
b0_n <- theta1[2]
b1_n <- theta1[3]
L1 <- likelihood_wei(t,d,w,a_n,b0_n,b1_n)
grad_norm1 <- norm(gradient_wei(t,d,w,a_n,b0_n,b1_n))
}
print(sprintf('%2.0f                 %2.0f          %3.4f               %.1e',
it,  halve, L1, grad_norm1))
halve = halve + 1
if(it == 1 | it ==2 | it == 499 | it == 500){
print(sprintf('%2.0f                 %2.0f          %3.4f               %.1e',it,  halve,L1, grad_norm1))
}
}
if(it == 1 | it == 2 | it == 499){
print("-----------------------------------------------------------------")
print(header)
}
r.e = max(abs(theta0 - theta1)/abs(pmax(1,abs(theta0))))
if (r.e < tolerr & grad_norm1 < tolgrad){stop == TRUE}
a <- a_n
b0 <- b0_n
b1 <- b1_n
it <- it + 1
}
return(list("estimator of alpha"=a, "estimator of beta_0" = b0, "estimator of beta_1" = b1, "iteration" = it))
}
newton2(t,d,w,1,1,1,500,1e-07,1e-07)
# newtons method
newton2 <- function(t,d,w,a,b0,b1,maxit,tolerr,tolgrad){
header = paste0("Iteration", "      halving", "     log-likelihood","      ||Gradient||")
print(header)
it = 1
stop = FALSE
while(it <= maxit & stop == FALSE){
# first steps
theta0 <- to_theta(a,b0,b1)
L0 <- likelihood_wei(t,d,w,a,b0,b1)
#grad elements
grad_0 <- gradient_wei(t,d,w,a,b0,b1)
grad_norm <- norm(grad_0)
grad_a <- grad_0[1]
grad_b0 <- grad_0[2]
grad_b1 <- grad_0[3]
#get direction
hess <- hessian_wei(t,d,w,a,b0,b1)
inv_h <- solve(hess)
direc <- (-1)*(inv_h %*% grad_0)
#print
print(sprintf('%2.0f                 --          %3.4f               %.1e',
it,  L0, grad_norm))
#get new params
theta1 = theta0 + direc
a_n <- theta1[1]
b0_n <- theta1[2]
b1_n <- theta1[3]
grad_norm1 <- gradient_wei(t,d,w,a_n,b0_n,b1_n)
if(theta1[1] > 0){
L1 <- likelihood_wei(t,d,w,a_n,b0_n,b1_n)
} else {L1 <- NaN}
halve <- 0
if(it == 1 | it ==2 | it == 499 | it == 500){
print(sprintf('%2.0f                 %2.0f          %3.4f               %.1e',it,  halve,L1, grad_norm1))
}
while(halve <= 20 & (theta1[1] <= 0 || L1 < L0)){
theta1 = theta0 + direc/(2^halve)
if(theta1[1] > 0){
a_n <- theta1[1]
b0_n <- theta1[2]
b1_n <- theta1[3]
L1 <- likelihood_wei(t,d,w,a_n,b0_n,b1_n)
grad_norm1 <- norm(gradient_wei(t,d,w,a_n,b0_n,b1_n))
}
print(sprintf('%2.0f                 %2.0f          %3.4f               %.1e',
it,  halve, L1, grad_norm1))
halve = halve + 1
if(it == 1 | it ==2 | it == 499 | it == 500){
print(sprintf('%2.0f                 %2.0f          %3.4f               %.1e',it,  halve,L1, grad_norm1))
}
}
if(it == 1 | it == 2 | it == 499){
print("-----------------------------------------------------------------")
print(header)
}
r.e = max(abs(theta0 - theta1)/abs(pmax(1,abs(theta0))))
if (r.e < tolerr & grad_norm1 < tolgrad){stop == TRUE}
a <- a_n
b0 <- b0_n
b1 <- b1_n
it <- it + 1
}
return(list("estimator of alpha"=a, "estimator of beta_0" = b0, "estimator of beta_1" = b1, "iteration" = it))
}
newton2(t,d,w,1,1,1,500,1e-07,1e-07)
