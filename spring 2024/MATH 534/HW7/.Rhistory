EZ = as.matrix(cbind(E1,E2,E3))
return(EZ)
}
#  EM algorithm
EM_alg <- function(alpha,beta,mu,lam,X,maxit){
# define N
N <- length(X)
# make prints
header = paste0("iteration ","alpha     ","beta      ","1-beta-alpha","mu       ","lambda    ","gradnorm")
# loop part
for(q in 1:maxit){
PI = c(alpha,beta,1-alpha-beta)
# get f1
f1 = rep(0,N)
for(i in 1:N){
if(X[i]==0){f1[i]=1}
}
# get f2 and f3
f2 = dpois(X,lambda = PI[2])
f3 = dpois(X,lambda = PI[3])
# run expectation update
expec(f1,f2,f3,PI[1],PI[2],PI[3]) -> EZ
#updates
alpha1 <- (1-beta)*sum(EZ[1])/sum(EZ[1]+EZ[3])
beta1 <- (1-alpha)*sum(EZ[2])/sum(EZ[2]+EZ[3])
mu1 <- sum(EZ[2]*X)/sum(EZ[2])
lam1 <- sum(EZ[3]*X)/sum(EZ[3])
theta1 <- c(alpha1,beta1,mu1,lam1)
gradnorm <- norm(LLgrad(theta1,X),"2")
# print
print(sprintf("%s    ","%f      ","%f      ","%f      ","%f      ","%f      ","%f      ",
q,
alpha1,
beta1,
1-alpha1-beta1,
mu1,
lam1,
gradnorm))
# resets
alpha1 -> alpha
beta1 -> beta
mu1 -> mu
lam1 -> lam
}
print(sprintf("The MLE's are alpha = %f, beta = %f, 1 - beta - alphha = %f, mu = %f, lambda = %f",
alpha,
beta,
1-beta-alpha,
mu,
lambda))
}
#run problem
data <- as.matrix(read.csv("Problem1_Data.csv", head =T))
EM_alg(1/3,1/3,1,30,data,30)
# making gradient of the llh function
LLgrad <- function(theta,X){
# take elements
n = length(X)
a = theta[1]
b = theta[2]
mu = theta[3]
lam = theta[4]
# input the calculations
dda = n/(a+b-1)
ddb = n*(1-a-2*b)/(b*(1-a-b))
ddmu = -n + (1/mu)*sum(X)
ddlam = -n + (1/lam)*sum(X)
# output
return(c(dda,ddb,ddmu,ddlam))
}
# expectation calculations
expec <- function(F1,F2,F3,P1,P2,P3){
nu1 = F1 * P1
nu2 = F2 * P2
nu3 = F3 * P3
D = nu1 + nu2 + nu3
E1 = nu1/D
E2 = nu2/D
E3 = nu3/D
EZ = as.matrix(cbind(E1,E2,E3))
return(EZ)
}
#  EM algorithm
EM_alg <- function(alpha,beta,mu,lam,X,maxit){
# define N
N <- length(X)
# make prints
header = paste0("iteration ","alpha     ","beta      ","1-beta-alpha","mu       ","lambda    ","gradnorm")
# loop part
for(q in 1:maxit){
PI = c(alpha,beta,1-alpha-beta)
# get f1
f1 = rep(0,N)
for(i in 1:N){
if(X[i]==0){f1[i]=1}
}
# get f2 and f3
f2 = dpois(X,lambda = PI[2])
f3 = dpois(X,lambda = PI[3])
# run expectation update
expec(f1,f2,f3,PI[1],PI[2],PI[3]) -> EZ
#updates
alpha1 <- (1-beta)*sum(EZ[1])/sum(EZ[1]+EZ[3])
beta1 <- (1-alpha)*sum(EZ[2])/sum(EZ[2]+EZ[3])
mu1 <- sum(EZ[2]*X)/sum(EZ[2])
lam1 <- sum(EZ[3]*X)/sum(EZ[3])
theta1 <- c(alpha1,beta1,mu1,lam1)
gradnorm <- norm(LLgrad(theta1,X),"2")
# print
print(sprintf("%s    ","%f      ","%f      ","%f      ","%f      ","%f      ","%f      ",
q,
alpha1,
beta1,
1-alpha1-beta1,
mu1,
lam1,
gradnorm))
# resets
alpha1 -> alpha
beta1 -> beta
mu1 -> mu
lam1 -> lam
}
print(sprintf("The MLE's are alpha = %f, beta = %f, 1 - beta - alphha = %f, mu = %f, lambda = %f",
alpha,
beta,
1-beta-alpha,
mu,
lam))
}
#run problem
data <- as.matrix(read.csv("Problem1_Data.csv", head =T))
EM_alg(1/3,1/3,1,30,data,30)
# make prints
#header = paste0("iteration ","alpha     ","beta      ","1-beta-alpha","mu       ","lambda    ","gradnorm")
ITER <-matrix(c(),nrow = 1,ncol = 7, byrow = T)
# making gradient of the llh function
LLgrad <- function(theta,X){
# take elements
n = length(X)
a = theta[1]
b = theta[2]
mu = theta[3]
lam = theta[4]
# input the calculations
dda = n/(a+b-1)
ddb = n*(1-a-2*b)/(b*(1-a-b))
ddmu = -n + (1/mu)*sum(X)
ddlam = -n + (1/lam)*sum(X)
# output
return(c(dda,ddb,ddmu,ddlam))
}
# expectation calculations
expec <- function(F1,F2,F3,P1,P2,P3){
nu1 = F1 * P1
nu2 = F2 * P2
nu3 = F3 * P3
D = nu1 + nu2 + nu3
E1 = nu1/D
E2 = nu2/D
E3 = nu3/D
EZ = as.matrix(cbind(E1,E2,E3))
return(EZ)
}
#  EM algorithm
EM_alg <- function(alpha,beta,mu,lam,X,maxit){
# define N
N <- length(X)
# make prints
#header = paste0("iteration ","alpha     ","beta      ","1-beta-alpha","mu       ","lambda    ","gradnorm")
ITER <-matrix(c(0,0,0,0,0,0,0),nrow = 1,ncol = 7, byrow = T)
# loop part
for(q in 1:maxit){
PI = c(alpha,beta,1-alpha-beta)
# get f1
f1 = rep(0,N)
for(i in 1:N){
if(X[i]==0){f1[i]=1}
}
# get f2 and f3
f2 = dpois(X,lambda = PI[2])
f3 = dpois(X,lambda = PI[3])
# run expectation update
expec(f1,f2,f3,PI[1],PI[2],PI[3]) -> EZ
#updates
alpha1 <- (1-beta)*sum(EZ[1])/sum(EZ[1]+EZ[3])
beta1 <- (1-alpha)*sum(EZ[2])/sum(EZ[2]+EZ[3])
mu1 <- sum(EZ[2]*X)/sum(EZ[2])
lam1 <- sum(EZ[3]*X)/sum(EZ[3])
theta1 <- c(alpha1,beta1,mu1,lam1)
gradnorm <- norm(LLgrad(theta1,X),"2")
# print
row <- c(q,alpha1,beta1,1-alpha1-beta1,mu1,lam1,gradnorm)
ITER <- rbind(ITER,row)
# resets
alpha1 -> alpha
beta1 -> beta
mu1 -> mu
lam1 -> lam
}
print(ITER)
print(sprintf("The MLE's are alpha = %f, beta = %f, 1 - beta - alphha = %f, mu = %f, lambda = %f",
alpha,
beta,
1-beta-alpha,
mu,
lam))
}
#run problem
data <- as.matrix(read.csv("Problem1_Data.csv", head =T))
EM_alg(1/3,1/3,1,30,data,30)
# making gradient of the llh function
LLgrad <- function(theta,X){
# take elements
n = length(X)
a = theta[1]
b = theta[2]
mu = theta[3]
lam = theta[4]
# input the calculations
dda = n/(a+b-1)
ddb = n*(1-a-2*b)/(b*(1-a-b))
ddmu = -n + (1/mu)*sum(X)
ddlam = -n + (1/lam)*sum(X)
# output
return(c(dda,ddb,ddmu,ddlam))
}
# expectation calculations
expec <- function(F1,F2,F3,P1,P2,P3){
nu1 = F1 * P1
nu2 = F2 * P2
nu3 = F3 * P3
D = nu1 + nu2 + nu3
E1 = nu1/D
E2 = nu2/D
E3 = nu3/D
EZ = as.matrix(cbind(E1,E2,E3))
return(EZ)
}
#  EM algorithm
EM_alg <- function(alpha,beta,mu,lam,X,maxit){
# define N
N <- length(X)
# make prints
#header = paste0("iteration ","alpha     ","beta      ","1-beta-alpha","mu       ","lambda    ","gradnorm")
ITER <-matrix(c(0,0,0,0,0,0,0),nrow = 1,ncol = 7, byrow = T)
# loop part
for(q in 1:maxit){
PI = c(alpha,beta,1-alpha-beta)
# get f1
f1 = rep(0,N)
for(i in 1:N){
if(X[i]==0){f1[i]=1}
}
# get f2 and f3
f2 = dpois(X,lambda = mu)
f3 = dpois(X,lambda = lam)
# run expectation update
expec(f1,f2,f3,PI[1],PI[2],PI[3]) -> EZ
#updates
alpha1 <- (1-beta)*sum(EZ[1])/sum(EZ[1]+EZ[3])
beta1 <- (1-alpha)*sum(EZ[2])/sum(EZ[2]+EZ[3])
mu1 <- sum(EZ[2]*X)/sum(EZ[2])
lam1 <- sum(EZ[3]*X)/sum(EZ[3])
theta1 <- c(alpha1,beta1,mu1,lam1)
gradnorm <- norm(LLgrad(theta1,X),"2")
# print
row <- c(q,alpha1,beta1,1-alpha1-beta1,mu1,lam1,gradnorm)
ITER <- rbind(ITER,row)
# resets
alpha1 -> alpha
beta1 -> beta
mu1 -> mu
lam1 -> lam
}
print(ITER)
print(sprintf("The MLE's are alpha = %f, beta = %f, 1 - beta - alphha = %f, mu = %f, lambda = %f",
alpha,
beta,
1-beta-alpha,
mu,
lam))
}
#run problem
data <- as.matrix(read.csv("Problem1_Data.csv", head =T))
EM_alg(1/3,1/3,1,30,data,30)
# making gradient of the llh function
LLgrad <- function(theta,X){
# take elements
n = length(X)
a = theta[1]
b = theta[2]
mu = theta[3]
lam = theta[4]
# input the calculations
dda = n/(a+b-1)
ddb = n*(1-a-2*b)/(b*(1-a-b))
ddmu = -n + (1/mu)*sum(X)
ddlam = -n + (1/lam)*sum(X)
# output
return(c(dda,ddb,ddmu,ddlam))
}
# expectation calculations
expec <- function(F1,F2,F3,P1,P2,P3){
nu1 = F1 * P1
nu2 = F2 * P2
nu3 = F3 * P3
D = nu1 + nu2 + nu3
E1 = nu1/D
E2 = nu2/D
E3 = nu3/D
EZ = as.matrix(cbind(E1,E2,E3))
return(EZ)
}
#  EM algorithm
EM_alg <- function(alpha,beta,mu,lam,X,maxit){
# define N
N <- length(X)
# make prints
#header = paste0("iteration ","alpha     ","beta      ","1-beta-alpha","mu       ","lambda    ","gradnorm")
ITER <-matrix(c(0,0,0,0,0,0,0),nrow = 1,ncol = 7, byrow = T)
# loop part
for(q in 1:maxit){
PI = c(alpha,beta,1-alpha-beta)
# get f1
f1 = rep(0,N)
for(i in 1:N){
if(X[i]==0){f1[i]=1}
}
# get f2 and f3
f2 = dpois(X,lambda = mu)
f3 = dpois(X,lambda = lam)
# run expectation update
expec(f1,f2,f3,PI[1],PI[2],PI[3]) -> EZ
#updates
alpha1 <- (1-beta)*sum(EZ[1])/sum(EZ[1]+EZ[3])
beta1 <- (1-alpha)*sum(EZ[2])/sum(EZ[2]+EZ[3])
mu1 <- sum(EZ[2]*X)/sum(EZ[2])
lam1 <- sum(EZ[3]*X)/sum(EZ[3])
theta1 <- c(alpha1,beta1,mu1,lam1)
gradnorm <- norm(LLgrad(theta1,X),"2")
# print
row <- c(q,alpha1,beta1,1-alpha1-beta1,mu1,lam1,gradnorm)
ITER <- rbind(ITER,row)
# resets
alpha1 -> alpha
beta1 -> beta
mu1 -> mu
lam1 -> lam
}
print(ITER)
print(sprintf("The MLE's are alpha = %f, beta = %f, 1 - beta - alphha = %f, mu = %f, lambda = %f",
alpha,
beta,
1-beta-alpha,
mu,
lam))
}
#run problem
data <- as.matrix(read.csv("Problem1_Data.csv", head =T))
EM_alg(1/3,1/3,1,30,data,30)
# making gradient of the llh function
LLgrad <- function(theta,X){
# take elements
n = length(X)
a = theta[1]
b = theta[2]
mu = theta[3]
lam = theta[4]
# input the calculations
dda = n/(a+b-1)
ddb = n*(1-a-2*b)/(b*(1-a-b))
ddmu = -n + (1/mu)*sum(X)
ddlam = -n + (1/lam)*sum(X)
# output
return(c(dda,ddb,ddmu,ddlam))
}
# expectation calculations
expec <- function(F1,F2,F3,P1,P2,P3){
nu1 = F1 * P1
nu2 = F2 * P2
nu3 = F3 * P3
D = nu1 + nu2 + nu3
E1 = nu1/D
E2 = nu2/D
E3 = nu3/D
EZ = as.matrix(cbind(E1,E2,E3))
return(EZ)
}
#  EM algorithm
EM_alg <- function(alpha,beta,mu,lam,X,maxit){
# define N
N <- length(X)
# make prints
#header = paste0("iteration ","alpha     ","beta      ","1-beta-alpha","mu       ","lambda    ","gradnorm")
ITER <-matrix(c(0,0,0,0,0,0,0),nrow = 1,ncol = 7, byrow = T)
# loop part
for(q in 1:maxit){
PI = c(alpha,beta,1-alpha-beta)
# get f1
f1 = rep(0,N)
for(i in 1:N){
if(X[i]==0){f1[i]=1}
}
# get f2 and f3
f2 = dpois(X,lambda = mu)
f3 = dpois(X,lambda = lam)
# run expectation update
expec(f1,f2,f3,PI[1],PI[2],PI[3]) -> EZ
#updates
alpha1 <- sum(EZ[1])/N
beta1 <- sum(EZ[2])/N
mu1 <- sum(EZ[2]*X)/sum(EZ[2])
lam1 <- sum(EZ[3]*X)/sum(EZ[3])
theta1 <- c(alpha1,beta1,mu1,lam1)
gradnorm <- norm(LLgrad(theta1,X),"2")
# print
row <- c(q,alpha1,beta1,1-alpha1-beta1,mu1,lam1,gradnorm)
ITER <- rbind(ITER,row)
# resets
alpha1 -> alpha
beta1 -> beta
mu1 -> mu
lam1 -> lam
}
print(ITER)
print(sprintf("The MLE's are alpha = %f, beta = %f, 1 - beta - alphha = %f, mu = %f, lambda = %f",
alpha,
beta,
1-beta-alpha,
mu,
lam))
}
#run problem
data <- as.matrix(read.csv("Problem1_Data.csv", head =T))
EM_alg(1/3,1/3,1,30,data,30)
# making gradient of the llh function
LLgrad <- function(theta,X){
# take elements
n = length(X)
a = theta[1]
b = theta[2]
mu = theta[3]
lam = theta[4]
# input the calculations
dda = n/(a+b-1)
ddb = n*(1-a-2*b)/(b*(1-a-b))
ddmu = -n + (1/mu)*sum(X)
ddlam = -n + (1/lam)*sum(X)
# output
return(c(dda,ddb,ddmu,ddlam))
}
# expectation calculations
expec <- function(F1,F2,F3,P1,P2,P3){
nu1 = F1 * P1
nu2 = F2 * P2
nu3 = F3 * P3
D = nu1 + nu2 + nu3
E1 = nu1/D
E2 = nu2/D
E3 = nu3/D
EZ = as.matrix(cbind(E1,E2,E3))
return(EZ)
}
#  EM algorithm
EM_alg <- function(alpha,beta,mu,lam,X,maxit){
# define N
N <- length(X)
# make prints
#header = paste0("iteration ","alpha     ","beta      ","1-beta-alpha","mu       ","lambda    ","gradnorm")
ITER <-matrix(c(0,0,0,0,0,0,0),nrow = 1,ncol = 7, byrow = T)
# loop part
for(q in 1:maxit){
PI = c(alpha,beta,1-alpha-beta)
# get f1
f1 = rep(0,N)
for(i in 1:N){
if(X[i]==0){f1[i]=1}
}
# get f2 and f3
f2 = dpois(X,lambda = mu)
f3 = dpois(X,lambda = lam)
# run expectation update
expec(f1,f2,f3,PI[1],PI[2],PI[3]) -> EZ
#updates
alpha1 <- sum(EZ[,1])/N
beta1 <- sum(EZ[,2])/N
mu1 <- sum(EZ[,2]*X)/sum(EZ[,2])
lam1 <- sum(EZ[,3]*X)/sum(EZ[,3])
theta1 <- c(alpha1,beta1,mu1,lam1)
gradnorm <- norm(LLgrad(theta1,X),"2")
# print
row <- c(q,alpha1,beta1,1-alpha1-beta1,mu1,lam1,gradnorm)
ITER <- rbind(ITER,row)
# resets
alpha1 -> alpha
beta1 -> beta
mu1 -> mu
lam1 -> lam
}
print(ITER)
print(sprintf("The MLE's are alpha = %f, beta = %f, 1 - beta - alphha = %f, mu = %f, lambda = %f",
alpha,
beta,
1-beta-alpha,
mu,
lam))
}
#run problem
data <- as.matrix(read.csv("Problem1_Data.csv", head =T))
EM_alg(1/3,1/3,1,30,data,30)
setwd("~/Desktop/githubbahubba/CSUF/spring 2024/MATH 534/HW7")
