g0 <- -ebx + sum(sales)
# component 2
for (i in 1:n) {
e_xbeta <- X[i,2]*exp(as.numeric(as.matrix(X[i,]) %*% beta))
ebx <- ebx + e_xbeta
}
g1 <- -ebx + sum(X[,2]*sales)
# component 3
for (i in 1:n) {
e_xbeta <- X[i,3]*exp(as.numeric(as.matrix(X[i,]) %*% beta))
ebx <- ebx + e_xbeta
}
g2 <- -ebx + sum(X[,3]*sales)
g = as.matrix(cbind(.6*g0,.6*g1,.6*g2),ncol=1)
list(ell = ell, g = g)
}
H <- Hess(y, X, beta_new)
H
# compute the gradient and Hessian
a <- like(y, X, beta_new, grad = TRUE)
g <- a$g
g
solve(H) %*% g
solve(H) %*% t(g)
g = as.matrix(cbind(.6*g0,.6*g1,.6*g2),ncol=1,nrow=3)
# likelihood function
like <- function(y, X, beta, grad = FALSE) {
# X is an n x p data frame consisting of predictors. If there is an intercept,
#    the first column of X is 1
# y is an n x 1 vector of responses
# beta is a p x 1 vector of coefficients
g = NULL
ell = NULL
tmp <- dim(X)
n <- tmp[1]
p <- tmp[2]
v = matrix(0, p, 1)
ebx <- 0
ebxx <- matrix(0, p, 1)
for (i in 1:n) {
v = v + t(X[i,]*y[i])
e_xbeta <- exp(as.numeric(as.matrix(X[i,]) %*% beta))
ebx <- ebx + e_xbeta
}
ell <- -ebx + t(v) %*% beta
# gen gradient
# component 1
g0 <- -ebx + sum(sales)
# component 2
for (i in 1:n) {
e_xbeta <- X[i,2]*exp(as.numeric(as.matrix(X[i,]) %*% beta))
ebx <- ebx + e_xbeta
}
g1 <- -ebx + sum(X[,2]*sales)
# component 3
for (i in 1:n) {
e_xbeta <- X[i,3]*exp(as.numeric(as.matrix(X[i,]) %*% beta))
ebx <- ebx + e_xbeta
}
g2 <- -ebx + sum(X[,3]*sales)
g = as.matrix(cbind(.6*g0,.6*g1,.6*g2),ncol=1,nrow=3)
list(ell = ell, g = g)
}
beta_new <- beta_old - solve(H) %*% g
# likelihood function
like <- function(y, X, beta, grad = FALSE) {
# X is an n x p data frame consisting of predictors. If there is an intercept,
#    the first column of X is 1
# y is an n x 1 vector of responses
# beta is a p x 1 vector of coefficients
g = NULL
ell = NULL
tmp <- dim(X)
n <- tmp[1]
p <- tmp[2]
v = matrix(0, p, 1)
ebx <- 0
ebxx <- matrix(0, p, 1)
for (i in 1:n) {
v = v + t(X[i,]*y[i])
e_xbeta <- exp(as.numeric(as.matrix(X[i,]) %*% beta))
ebx <- ebx + e_xbeta
}
ell <- -ebx + t(v) %*% beta
# gen gradient
# component 1
g0 <- -ebx + sum(sales)
# component 2
for (i in 1:n) {
e_xbeta <- X[i,2]*exp(as.numeric(as.matrix(X[i,]) %*% beta))
ebx <- ebx + e_xbeta
}
g1 <- -ebx + sum(X[,2]*sales)
# component 3
for (i in 1:n) {
e_xbeta <- X[i,3]*exp(as.numeric(as.matrix(X[i,]) %*% beta))
ebx <- ebx + e_xbeta
}
g2 <- -ebx + sum(X[,3]*sales)
g = as.matrix(cbind(.6*g0,.6*g1,.6*g2),ncol=1,nrow=3)
list(ell = ell, g = g)
}
Hess <- function(y, X, beta) {
# X is an n x p matrix of predictors
# y is an n x 1 vector of responses
# beta is a p x 1 vector of coefficients
tmp <- dim(X)
n <- tmp[1]
p <- tmp[2]
H <- matrix(0, p, p)
for( i in 1:n){
e_xbeta <- exp(as.matrix(X[i,]) %*% beta)
for (k in 1:p) {
for(m in 1:p) {
H[k,m] <- H[k,m] - e_xbeta * X[i,k] * X[i,m]
}
}
}
return(H)
}
#create Newton's method for maximization
newton <- function(y, X, beta, maxit = 100, tol_err = 1e-9, tol_grad = 1e-9) {
# X is an n x p matrix of predictors
# y is an n x 1 vector of responses
# beta is a p x 1 vector of coefficients
# maxit is the maximum number of iterations
# tol is the convergence toleranc
# initialize the parameters
tmp <- dim(X)
n <- tmp[1]
p <- tmp[2]
beta_old <- beta
beta_new <- beta
# iterate until convergence
for (iter in 1:maxit) {
# compute the gradient and Hessian
a <- like(y, X, beta_new, grad = TRUE)
g <- a$g
H <- Hess(y, X, beta_new)
# update the parameters
beta_old <- beta_new
beta_new <- beta_old - solve(H) %*% t(g)
# Add your code for checking convergence here
mre = max(abs(beta_old - beta_new)/abs(pmax(1,abs(beta_old))))
if (mre < tol_err & norm(a$g) < tol_grad){break}
# Keep the print statement below for your final version
print(c(iter, beta_new, a$ell, norm(a$g, "2"), mre))
}
# return the estimated parameters
return(beta_new)
}
newton(y,X,c(.1,.1,.1))
d = c(1,1,-1)
grad1 <- like(y, X, c(1,1,1), grad = T)$g
d %*% grad1
d = c(1,1,-1)
grad1 <- like(y, X, c(1,1,1), grad = T)$g
d %*% grad1
d = c(1,1,-1)
grad1 <- like(y, X, c(1,1,1), grad = T)$g
d %*% t(grad1)
# likelihood function
like <- function(y, X, beta, grad = FALSE) {
# X is an n x p data frame consisting of predictors. If there is an intercept,
#    the first column of X is 1
# y is an n x 1 vector of responses
# beta is a p x 1 vector of coefficients
g = NULL
ell = NULL
tmp <- dim(X)
n <- tmp[1]
p <- tmp[2]
v = matrix(0, p, 1)
ebx <- 0
ebxx <- matrix(0, p, 1)
for (i in 1:n) {
v = v + t(X[i,]*y[i])
e_xbeta <- exp(as.numeric(as.matrix(X[i,]) %*% beta))
ebx <- ebx + e_xbeta
}
ell <- -ebx + t(v) %*% beta
# gen gradient
# component 1
g0 <- -ebx + sum(sales)
# component 2
for (i in 1:n) {
e_xbeta <- X[i,2]*exp(as.numeric(as.matrix(X[i,]) %*% beta))
ebx <- ebx + e_xbeta
}
g1 <- -ebx + sum(X[,2]*y)
# component 3
for (i in 1:n) {
e_xbeta <- X[i,3]*exp(as.numeric(as.matrix(X[i,]) %*% beta))
ebx <- ebx + e_xbeta
}
g2 <- -ebx + sum(X[,3]*y)
g = as.matrix(cbind(.6*g0,.6*g1,.6*g2),ncol=1,nrow=3)
list(ell = ell, g = t(g))
}
like(y, X, c(0.1,.1,.1), grad = T)
d = c(1,1,-1)
grad1 <- like(y, X, c(1,1,1), grad = T)$g
d %*% t(grad1)
d = c(1,1,-1)
grad1 <- like(y, X, c(1,1,1), grad = T)$g
d %*% grad1
# likelihood function
like <- function(y, X, beta, grad = FALSE) {
# X is an n x p data frame consisting of predictors. If there is an intercept,
#    the first column of X is 1
# y is an n x 1 vector of responses
# beta is a p x 1 vector of coefficients
g = NULL
ell = NULL
tmp <- dim(X)
n <- tmp[1]
p <- tmp[2]
v = matrix(0, p, 1)
ebx <- 0
ebxx <- matrix(0, p, 1)
for (i in 1:n) {
v = v + t(X[i,]*y[i])
e_xbeta <- exp(as.numeric(as.matrix(X[i,]) %*% beta))
ebx <- ebx + e_xbeta
}
ell <- -ebx + t(v) %*% beta
# gen gradient
# component 1
g0 <- -ebx + sum(sales)
# component 2
for (i in 1:n) {
e_xbeta <- X[i,2]*exp(as.numeric(as.matrix(X[i,]) %*% beta))
ebx <- ebx + e_xbeta
}
g1 <- -ebx + sum(X[,2]*sales)
# component 3
for (i in 1:n) {
e_xbeta <- X[i,3]*exp(as.numeric(as.matrix(X[i,]) %*% beta))
ebx <- ebx + e_xbeta
}
g2 <- -ebx + sum(X[,3]*sales)
g = as.matrix(cbind(.6*g0,.6*g1,.6*g2),ncol=1,nrow=3)
list(ell = ell, g = t(g))
}
Hess <- function(y, X, beta) {
# X is an n x p matrix of predictors
# y is an n x 1 vector of responses
# beta is a p x 1 vector of coefficients
tmp <- dim(X)
n <- tmp[1]
p <- tmp[2]
H <- matrix(0, p, p)
for( i in 1:n){
e_xbeta <- exp(as.matrix(X[i,]) %*% beta)
for (k in 1:p) {
for(m in 1:p) {
H[k,m] <- H[k,m] - e_xbeta * X[i,k] * X[i,m]
}
}
}
return(H)
}
#create Newton's method for maximization
newton <- function(y, X, beta, maxit = 100, tol_err = 1e-9, tol_grad = 1e-9) {
# X is an n x p matrix of predictors
# y is an n x 1 vector of responses
# beta is a p x 1 vector of coefficients
# maxit is the maximum number of iterations
# tol is the convergence toleranc
# initialize the parameters
tmp <- dim(X)
n <- tmp[1]
p <- tmp[2]
beta_old <- beta
beta_new <- beta
# iterate until convergence
for (iter in 1:maxit) {
# compute the gradient and Hessian
a <- like(y, X, beta_new, grad = TRUE)
g <- a$g
H <- Hess(y, X, beta_new)
# update the parameters
beta_old <- beta_new
beta_new <- beta_old - solve(H) %*% t(g)
# Add your code for checking convergence here
mre = max(abs(beta_old - beta_new)/abs(pmax(1,abs(beta_old))))
if (mre < tol_err & norm(a$g) < tol_grad){break}
# Keep the print statement below for your final version
print(c(iter, beta_new, a$ell, norm(a$g, "2"), mre))
}
# return the estimated parameters
return(beta_new)
}
newton(y,X,c(.1,.1,.1))
# likelihood function
like <- function(y, X, beta, grad = FALSE) {
# X is an n x p data frame consisting of predictors. If there is an intercept,
#    the first column of X is 1
# y is an n x 1 vector of responses
# beta is a p x 1 vector of coefficients
g = NULL
ell = NULL
tmp <- dim(X)
n <- tmp[1]
p <- tmp[2]
v = matrix(0, p, 1)
ebx <- 0
ebxx <- matrix(0, p, 1)
for (i in 1:n) {
v = v + t(X[i,]*y[i])
e_xbeta <- exp(as.numeric(as.matrix(X[i,]) %*% beta))
ebx <- ebx + e_xbeta
}
ell <- -ebx + t(v) %*% beta
# gen gradient
# component 1
g0 <- -ebx + sum(sales)
# component 2
for (i in 1:n) {
e_xbeta <- X[i,2]*exp(as.numeric(as.matrix(X[i,]) %*% beta))
ebx <- ebx + e_xbeta
}
g1 <- -ebx + sum(X[,2]*sales)
# component 3
for (i in 1:n) {
e_xbeta <- X[i,3]*exp(as.numeric(as.matrix(X[i,]) %*% beta))
ebx <- ebx + e_xbeta
}
g2 <- -ebx + sum(X[,3]*sales)
g = as.matrix(cbind(.6*g0,.6*g1,.6*g2),ncol=1,nrow=3)
list(ell = ell, g = t(g))
}
Hess <- function(y, X, beta) {
# X is an n x p matrix of predictors
# y is an n x 1 vector of responses
# beta is a p x 1 vector of coefficients
tmp <- dim(X)
n <- tmp[1]
p <- tmp[2]
H <- matrix(0, p, p)
for( i in 1:n){
e_xbeta <- exp(as.matrix(X[i,]) %*% beta)
for (k in 1:p) {
for(m in 1:p) {
H[k,m] <- H[k,m] - e_xbeta * X[i,k] * X[i,m]
}
}
}
return(H)
}
#create Newton's method for maximization
newton <- function(y, X, beta, maxit = 100, tol_err = 1e-9, tol_grad = 1e-9) {
# X is an n x p matrix of predictors
# y is an n x 1 vector of responses
# beta is a p x 1 vector of coefficients
# maxit is the maximum number of iterations
# tol is the convergence toleranc
# initialize the parameters
tmp <- dim(X)
n <- tmp[1]
p <- tmp[2]
beta_old <- beta
beta_new <- beta
# iterate until convergence
for (iter in 1:maxit) {
# compute the gradient and Hessian
a <- like(y, X, beta_new, grad = TRUE)
g <- a$g
H <- Hess(y, X, beta_new)
# update the parameters
beta_old <- beta_new
beta_new <- beta_old - solve(H) %*% (g)
# Add your code for checking convergence here
mre = max(abs(beta_old - beta_new)/abs(pmax(1,abs(beta_old))))
if (mre < tol_err & norm(a$g) < tol_grad){break}
# Keep the print statement below for your final version
print(c(iter, beta_new, a$ell, norm(a$g, "2"), mre))
}
# return the estimated parameters
return(beta_new)
}
newton(y,X,c(.1,.1,.1))
# likelihood function
like <- function(y, X, beta, grad = FALSE) {
# X is an n x p data frame consisting of predictors. If there is an intercept,
#    the first column of X is 1
# y is an n x 1 vector of responses
# beta is a p x 1 vector of coefficients
g = NULL
ell = NULL
tmp <- dim(X)
n <- tmp[1]
p <- tmp[2]
v = matrix(0, p, 1)
ebx <- 0
ebxx <- matrix(0, p, 1)
for (i in 1:n) {
v = v + t(X[i,]*y[i])
e_xbeta <- exp(as.numeric(as.matrix(X[i,]) %*% beta))
ebx <- ebx + e_xbeta
}
ell <- -ebx + t(v) %*% beta
# gen gradient
# component 1
g0 <- -ebx + sum(sales)
# component 2
for (i in 1:n) {
e_xbeta <- X[i,2]*exp(as.numeric(as.matrix(X[i,]) %*% beta))
ebx <- ebx + e_xbeta
}
g1 <- -ebx + sum(X[,2]*y)
# component 3
for (i in 1:n) {
e_xbeta <- X[i,3]*exp(as.numeric(as.matrix(X[i,]) %*% beta))
ebx <- ebx + e_xbeta
}
g2 <- -ebx + sum(X[,3]*y)
g = as.matrix(cbind(g0,g1,g2),ncol=1,nrow=3)
list(ell = ell, g = t(g))
}
like(y, X, c(0.1,.1,.1), grad = T)
d = c(1,1,-1)
grad1 <- like(y, X, c(1,1,1), grad = T)$g
d %*% grad1
# likelihood function
like <- function(y, X, beta, grad = FALSE) {
# X is an n x p data frame consisting of predictors. If there is an intercept,
#    the first column of X is 1
# y is an n x 1 vector of responses
# beta is a p x 1 vector of coefficients
g = NULL
ell = NULL
tmp <- dim(X)
n <- tmp[1]
p <- tmp[2]
v = matrix(0, p, 1)
ebx <- 0
ebxx <- matrix(0, p, 1)
for (i in 1:n) {
v = v + t(X[i,]*y[i])
e_xbeta <- exp(as.numeric(as.matrix(X[i,]) %*% beta))
ebx <- ebx + e_xbeta
}
ell <- -ebx + t(v) %*% beta
# gen gradient
# component 1
g0 <- -ebx + sum(sales)
# component 2
for (i in 1:n) {
e_xbeta <- X[i,2]*exp(as.numeric(as.matrix(X[i,]) %*% beta))
ebx <- ebx + e_xbeta
}
g1 <- -ebx + sum(X[,2]*sales)
# component 3
for (i in 1:n) {
e_xbeta <- X[i,3]*exp(as.numeric(as.matrix(X[i,]) %*% beta))
ebx <- ebx + e_xbeta
}
g2 <- -ebx + sum(X[,3]*sales)
g = as.matrix(cbind(.6*g0,.6*g1,.6*g2),ncol=1,nrow=3)
list(ell = ell, g = t(g))
}
Hess <- function(y, X, beta) {
# X is an n x p matrix of predictors
# y is an n x 1 vector of responses
# beta is a p x 1 vector of coefficients
tmp <- dim(X)
n <- tmp[1]
p <- tmp[2]
H <- matrix(0, p, p)
for( i in 1:n){
e_xbeta <- exp(as.matrix(X[i,]) %*% beta)
for (k in 1:p) {
for(m in 1:p) {
H[k,m] <- H[k,m] - e_xbeta * X[i,k] * X[i,m]
}
}
}
return(H)
}
#create Newton's method for maximization
newton <- function(y, X, beta, maxit = 100, tol_err = 1e-9, tol_grad = 1e-9) {
# X is an n x p matrix of predictors
# y is an n x 1 vector of responses
# beta is a p x 1 vector of coefficients
# maxit is the maximum number of iterations
# tol is the convergence toleranc
# initialize the parameters
tmp <- dim(X)
n <- tmp[1]
p <- tmp[2]
beta_old <- beta
beta_new <- beta
# iterate until convergence
for (iter in 1:maxit) {
# compute the gradient and Hessian
a <- like(y, X, beta_new, grad = TRUE)
g <- a$g
H <- Hess(y, X, beta_new)
# update the parameters
beta_old <- beta_new
beta_new <- beta_old - solve(H) %*% (g)
# Add your code for checking convergence here
mre = max(abs(beta_old - beta_new)/abs(pmax(1,abs(beta_old))))
if (mre < tol_err & norm(a$g) < tol_grad){break}
# Keep the print statement below for your final version
print(c(iter, beta_new, a$ell, norm(a$g, "2"), mre))
}
# return the estimated parameters
return(beta_new)
}
newton(y,X,c(.1,.1,.1))
