---
title: "homework 3 (part 2)"
author: "Michael Pena"
date: "2024-02-20"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(matrixcalc)
library(dplyr)
library(tidyverse)
```

## part (a).

```{r}
#function to square root a matrix "A"
sqrtm <- function(A){
  a <- eigen(A)
  sqm <- a$vectors %*% diag(sqrt(a$values)) %*% t(a$vectors)
  sqm <- (sqm+t(sqm))/2
}

#function for generating data
gen <- function(n,p,mu,sigma,seed){
  #generate data from a p-variate normal with mean mu and covaraince sigma
  #set seed to 2024
  set.seed(seed)
  #generate data from normal
  z <- matrix(rnorm(n*p),n,p)
  datan <- z %*% sqrtm(sigma) + matrix(mu,n,p,byrow = TRUE)
  return(datan)
}
```

```{r}
# putting in the data
sig <- matrix(c(1,0.7,0.7,0.7,1,0.7,0.7,0.7,1), nrow = 3, ncol = 3)
mu <- matrix(c(-1,1,2), nrow =3)

gen(200,3,mu,sig,2024)


```




## part (b).
 
```{r}
# compile Sigma and Mu into a single theta vector
to.theta <- function(mu,sig){
  p <- length(mu)
  theta <- matrix(0,nrow = (p + p*(1+p)/2),ncol = 1)
  theta[1:p] <- mu

  k = p + 1
  for(i in 1:p){
    for(j in 1:i){
      theta[k] <- sig[i,j]
      k = k + 1
    }
  }
  return(theta)
}
```


```{r}
# turning theta back into mu and Sigma
from.theta <- function(p,theta){
  mu <- theta[1:p]
  sig <- matrix(0, nrow = p, ncol = p)
  
  k = p + 1
  
  for (i in 1:p){
    for (j in 1:i){
      sig[i,j] <- theta[k]
      sig[j,i] <- sig[i,j]
      k = k + 1
    }
  }
 list(mu = mu, sig = sig) 
}
```

 
```{r}
# make gradient
gradient <- function(x,mu,sig){
  n <- nrow(x)
  p <- ncol(x)
  
  inv.sig <- solve(sig) # inverse sigma
  
  # make initials
  xi.sum <- matrix(0, nrow = p, ncol = 1)
  grad.mu <- xi.sum
  C.mu <- matrix(0, nrow = p, ncol = p)
  
  # take care of C.mu
  for (i in 1:n){
    xi <- as.numeric(x[i,] - mu)
    xi.sum <- xi.sum + xi
    C.mu <- C.mu + (xi %*% t(xi))
  }
  grad.mu <- inv.sig %*% xi.sum
  A <- (n* inv.sig) - inv.sig %*% C.mu  %*% inv.sig
  grad.sig <- matrix(0, nrow = nrow(A), ncol = ncol(A))
  
  for (i in 1:nrow(sig) - 1){
    grad.sig[i,i] <- -0.5*A[i,i]
  }
  for (i in 1:nrow(sig) - 1){
    for (j in (i+1):ncol(sig)){
      grad.sig[i,j] <- -1*A[i,j]
      grad.sig[j,i] <- -1*A[i,j]
    }
  }
  grad.norm <- norm(to.theta(grad.mu,grad.sig), type = '2')
  list(grad.mu = grad.mu, grad.sig=grad.sig, grad.norm = grad.norm)
}

```

 
```{r}
likemvn <- function (x,mu,sig) {
  # computes the likelihood and the gradient for multivariate normal
  # if gcomp=FALSE, then the gradient is not computed
  # x is the n by p data matrix
  # mu is the mean
  # sig is the covariance
  # gcomp if TRUE, the gradient with respect to mu will be output
  n = nrow(x)
  p = ncol(x)

  C.mu = matrix(0,p,p) # initializing sum of (xi-mu)(xi-mu)^T
  xi.sum = matrix(0,p,1) # initializing sum of xi-mu
  grad.mu = xi.sum; # initializing this sum is used for the gradient w.r.t. mu
  for (i in 1:n){
    xi = as.numeric(x[i,] - mu)
    xi = xi + 1
    C.mu = C.mu + xi %*% t(xi)
  }

  ell = -(n*p*log(2*pi)+n*log(det(sig)) + sum(solve(sig) %*% C.mu ))/2
  return(ell)
}

```

```{r}
# Steepest ascent

optmvn <- function (x,mu,sig,maxit,tolerr,tolgrad) {
  header = paste0("Iteration",
                  "      Halving",
                  "       log-likelihood",
                  "       ||gradient||")
  print(header)
  
  for(it in 1:maxit){
    theta0 <- to.theta(mu,sig)
    L <- likemvn(x,mu,sig)
    grad.mu0 <- gradient(x,mu,sig)$grad.mu
    grad.sig0 <- gradient(x,mu,sig)$grad.sig
    grad.norm0 <- gradient(x,mu,sig)$grad.norm
    
    if (it == 1 | it ==2 | it == 477 | it == 478){
      print(sprintf('%2.0f                 --          %3.4f               %.1e',it,L,grad.norm0))
    }
    
    direc <- to.theta(grad.mu0,grad.sig0) # get direction
    # get new components
    theta1 <-  theta0 + direc
    mu1 <- from.theta(length(mu), theta1)$mu
    sig1 <- from.theta(length(mu), theta1)$sig
    grad.norm1 <- gradient(x,mu1,sig1)$grad.norm
    
    if(all(eigen(sig1)$values > 0)){atmp = likemvn(x,mu1,sig1)}
    else{atmp = -Inf}
    
    halve = 0
    
    if(it == 1 | it ==2 | it == 477 | it == 478){
    print(sprintf('%2.0f                 %2.0f          %3.4f               %.1e', 
                  it,  halve, atmp, grad.norm1))}
    
    while((all(eigen(sig1)$values <= 0) && halve < 20) || atmp < L){
      halve = halve + 1
      # mathematics
      theta1 <- theta0 + direc/(2^halve)
      mu1 = from.theta(length(mu), theta1)$mu
      sig1 = from.theta(length(mu), theta1)$sig
      
      if(all(eigen(sig1)$values > 0)){atmp = likemvn(x,mu1,sig1)}
      else{atmp = -Inf}
      
      grad.norm1 <-  gradient(x,mu1,sig1)$grad.norm
      
      if(it == 1 | it ==2 | it == 477 | it == 478){
          print(sprintf('%2.0f                 %2.0f          %3.4f               %.1e',it,  halve,atmp, grad.norm1))
      }
    }
    if(it == 1 | it == 2 | it == 477){
      print("-----------------------------------------------------------------")
      print(header)
    }
    r.e <- max(abs(theta0 - theta1)/abs(pmax(1,abs(theta0))))
    theta0 <- theta1
    
    if (r.e < tolerr & grad.norm1 < tolgrad){break}
    
    mu <- mu1
    sig <- sig1
  }
  return(list("mu.estimator" = mu,
              "sigma.estimator" = sig,
              "iteration" = it))
}
```

```{r, warning=FALSE}
# putting in parameters
x <- gen(200,3,mu,sig,2024)
m <- c(0,0,0)
s <- diag(3)

optmvn(x,m,s,500,1e-6,1e-5)
```



