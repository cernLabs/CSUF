# Non-Parametric Bootstrapped Regression (Kernel Regression with Bootstrap Resampling)# Data Preparation and Explorationsetwd("/Users/ann/Downloads")data <- read.csv("KS example.csv", header = TRUE)head(data) #to check matchattach(data) #check!!!!!!!!!!!!!x = compression # Predictor xy = energy # Response yplot(x,y) #same as plot(data$compression, data$energy)# (STEP 0): Cross Validation for Optimal Bandwidth (h) that minimizes the sum of squared residualsdata = data.frame(x,y)n = length(x)CV = function(h){  SSR = 0  for(i in 1:n){    y1 = sum(exp(-.5*((x[i]-x[-i])/h)^2)*y[-i])/sum(exp(-.5*((x[i]-x[-i])/h)^2))    resid = y[i] - y1    SSR = SSR + resid^2  }  SSR}h=optim(1,CV)$par #h = 0.74685# [Diagnostic plot] Prof Provided us code# Calculating residuals and creating a diagnostic plot to assess the model's performance.# x1 are user defined values we want to input to the model# For now, x1 goes from smallest to largest x in small increments for plotting# y1 is what you get out of the model when you plug in x1# ?Original Residuals#(Step 1) Obtain Residualsx1 = x #setting x1 equals compressiony1 = rep(0,length(x1))for(i in 1:length(x1)){  y1[i] = sum(exp(-.5*((x1[i]-x)/h)^2)*y)/sum(exp(-.5*((x1[i]-x)/h)^2))}resid = y- y1 #Calculate the residuals (resid) by subtracting the predicted values (y1) from the actual values (y)# (console) resid # (console) lines(x1,y1,col=2)# (console) qqnorm(resid) : to check my residuals are normal#           qqnorm(resid) : Normal Q-Q Plot as outputplot(x, resid) # a diagnostic plot to visualize the residuals.# ? y|x* = exp(-.5*((x1[i]-x)/h)^2)*y representing how many data points I have qqnorm(resid)#why???x1 = 10 #putting into non bootstrap model:y1 = rep(0,length(x1))for(i in 1:length(x1)){  y1[i] = sum(exp(-.5*((x1[i]-x)/h)^2)*y)/sum(exp(-.5*((x1[i]-x)/h)^2))}#(Step 2) Generate BS.x from predictor variabl xBS.x = sample(x,n,replace=T)#(Step 3) Getting BS.yhat by plugging in BS.x to original model# For each value in the bootstrap sample (BS.x), you calculate the predicted values (y1) using Kernel Regression.# Reflect the variability in that estimateso generate BS.yhat using original model # (plugging in BS.x to orig model-Kernel Density Est Code to get yhat - using Kernel reg model)x1 = BS.xy1 = rep(0,length(x1)) # output: y1 (note: same number as length(x1))# rep(0,length(x1)) is bunch of zero vector spacesfor(i in 1:length(x1)){  y1[i] = sum(exp(-.5*((x1[i]-x)/h)^2)*y)/sum(exp(-.5*((x1[i]-x)/h)^2))   # x represents x1 throught xn  # -x = minus my entire vector for x doing all of them at once   # I have n differences are squared}#lines(x1,y1,col=3, lwd=2) #Green line is my est. of red lineBS.yhat=y1#(Step 4) Generate BS.y by adding residuals to BS.yhat# Generating Bootstrap Samples of Response Variable# Adding the original residuals (resid) to the predicted values (y1) to obtain bootstrap samples of the response variable (BS.y).BS.y = BS.yhat + sample(resid,n,replace=T)par(mfrow=c(2,1)) #show graph as oneplot(x,y)plot(BS.x,BS.y)# (Step 5) New Model: Kernel Regression using BS.x and BS.y [hard coding original data]!!# Challenge Starts = Kernel Regression using Bootstrap Samples...You can do this!!# (Step 6) plug x1=10 into new model. Store result. # This code will calculate the predicted value of y for x1 = 10 using the Kernel Regression model # with the bootstrap samples BS.x and BS.y, and store the result in y1.#11111111. plug x1=10 into new modelx1 = 10 y1_10 = rep(0,length(x1)) #output# Kernel Regression model using bootstrap samplesfor(i in 1:length(x1)){  # y1[i] = sum(exp(-.5*((x1[i]-x)/h)^2)*y)/sum(exp(-.5*((x1[i]-x)/h)^2)) ###  # x represents x1 throught xn  # -x = minus my entire vector for x doing all of them at once   # I have n differences are squared  #here is the kernel regression model  y1_10[i] = sum(exp(-.5*((x1[i]-BS.x)/h)^2)*BS.y)/sum(exp(-.5*((x1[i]-BS.x)/h)^2))   # which is y|x*?????????????????????????}#222222plug x1=0.1 into new modelx1 = 0.1y1_0.1 = rep(0,length(x1)) #output# Kernel Regression model using bootstrap samplesfor(i in 1:length(x1)){  # y1[i] = sum(exp(-.5*((x1[i]-x)/h)^2)*y)/sum(exp(-.5*((x1[i]-x)/h)^2)) ###  # x represents x1 throught xn  # -x = minus my entire vector for x doing all of them at once   # I have n differences are squared  #here is the kernel regression model  y1_0.1[i] = sum(exp(-.5*((x1[i]-BS.x)/h)^2)*BS.y)/sum(exp(-.5*((x1[i]-BS.x)/h)^2))   # which is y|x*}#33333plug x1=1 into new modelx1 = 1y1_1 = rep(0,length(x1)) #output# Kernel Regression model using bootstrap samplesfor(i in 1:length(x1)){  # y1[i] = sum(exp(-.5*((x1[i]-x)/h)^2)*y)/sum(exp(-.5*((x1[i]-x)/h)^2)) ###  # x represents x1 throught xn  # -x = minus my entire vector for x doing all of them at once   # I have n differences are squared  #here is the kernel regression model  y1_1[i] = sum(exp(-.5*((x1[i]-BS.x)/h)^2)*BS.y)/sum(exp(-.5*((x1[i]-BS.x)/h)^2))   # which is y|x*}#plug x1 into new model and get y1 y1  # Output the resulty1_10y1_0.1y1_1############################################################################################################################################################## (Step 7-1) Repeat 10,000 times n_iterations = 10000  # Number of iterationsy1_values = numeric(n_iterations)  # Initialize vector to store y1 values# Loop over each iterationfor (iteration in 1:n_iterations) {  # Step 2: Generate bootstrap sample BS.x from original predictor variable x  BS.x = sample(x, n, replace = TRUE)    # Step 3: Compute predicted values BS.yhat using Kernel Regression  y1 = rep(0, length(BS.x))  # Initialize vector to store predicted values  for (i in 1:length(BS.x)) {    # Compute predicted value for each data point in the bootstrap sample    y1[i] = sum(exp(-.5 * ((BS.x[i] - x) / h)^2) * y) / sum(exp(-.5 * ((BS.x[i] - x) / h)^2))  }  BS.yhat = y1  # Store predicted values    # Step 4: Generate BS.y by adding residuals to BS.yhat  BS.y = BS.yhat + sample(resid,n,replace=T)    # Step 6: Kernel Regression using BS.x and BS.y  # Plug x1 = 10 into the new model and store result  x1 = 10  y1 = sum(exp(-.5 * ((x1 - BS.x) / h)^2) * BS.y) / sum(exp(-.5 * ((x1 - BS.x) / h)^2))    # Store x1 and y1 values for each iteration  y1_values[iteration] = y1}# Step 8: Find alpha/2 and 1-(alpha/2) cut-offsalpha = 0.05  # Significance levellower_cutoff = quantile(y1_values, alpha / 2)  # Compute lower cut-offupper_cutoff = quantile(y1_values, 1 - (alpha / 2))  # Compute upper cut-off# Output cut-off valuescat("Lower cut-off:", lower_cutoff, "\n")cat("Upper cut-off:", upper_cutoff, "\n")# Lower cut-off: 11.82987 ***************Good????# Upper cut-off: 11.90703 # Residual Diagnostic Plot **************ASK!!!!!!!!!!!!!qqnorm(resid)