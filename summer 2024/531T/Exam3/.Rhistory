model.a = randomForest(description ~ plate_x + plate_z + balls + strikes + release_speed + home_score + away_score,
data = train.data, ntree = ntree.in, mtry = 3, control=rpart.control(minsplit=ms.in,cp=cp.in))
model.b = boosting(description ~ plate_x + plate_z + balls + strikes + release_speed + home_score + away_score,
data = train.data, mfinal=mfinal.in, control=rpart.control(minsplit=ms.in,cp=cp.in))
set.seed(536)
n = dim(zone)[1]
n
CVLL.a = 0
CVLL.b = 0
CV.ind.mat = matrix(sample(1:n,3070,replace=F),nrow=5)
head(zone)
CVLL.a = 0
CVLL.b = 0
k=1
train.data = zone[(1:n)[-CV.ind.mat[k,]],]
test.data = zone[CV.ind.mat[k,],]
dim(train.data)
dim(test.data)
model.a = randomForest(description ~ plate_x + plate_z + balls + strikes + release_speed + home_score + away_score,
data = train.data, ntree = ntree.in, mtry = 3, control=rpart.control(minsplit=ms.in,cp=cp.in))
model.b = boosting(description ~ plate_x + plate_z + balls + strikes + release_speed + home_score + away_score,
data = train.data, mfinal=mfinal.in, control=rpart.control(minsplit=ms.in,cp=cp.in))
y = (as.numeric(zone$description) - 1)[CV.ind.mat[k,]]
p.a = predict(model.a,newdata=test.data,type="prob")[,2]
p.b = predict(model.b,newdata=test.data)$prob[,2]
p.b[p.b==0] = 0.00001
p.b[p.b==1] = .9999
LL.a1 = sum(y*log(p.a) + (1-y)*log(1-p.a))
LL.b1 = sum(y*log(p.b) + (1-y)*log(1-p.b))
CVLL.a = CVLL.a + LL.a1
CVLL.b = CVLL.b + LL.b1
FiveFold <- function(ntree.in, mfinal.in, cp.in, ms.in){
###model b wins by a mile, but I'm a little worried about overfitting.  Time to cross validate)
###Going to 10 fold cross validate
set.seed(536)
n = dim(zone)[1]
###with 179 data points, going to have 17 data points in each testing set
###Obviously that means there are 9 data points that never get partitioned
###as testing data.  But we can pick those up as their own testing group at the end if we want to.
CV.ind.mat = matrix(sample(1:n,3070,replace=F),nrow=5)
#Run same code as before, but each time, we're going to run our models using only train data
#and testing them using only testing data.
CVLL.a = 0
CVLL.b = 0
for(k in 1:5){
train.data = zone[(1:n)[-CV.ind.mat[k,]],]
test.data = zone[CV.ind.mat[k,],]
model.a = randomForest(description ~ plate_x + plate_z + balls + strikes + release_speed + home_score + away_score,
data = train.data, ntree = ntree.in, mtry = 3, control=rpart.control(minsplit=ms.in,cp=cp.in))
model.b = boosting(description ~ plate_x + plate_z + balls + strikes + release_speed + home_score + away_score,
data = train.data, mfinal=mfinal.in, control=rpart.control(minsplit=ms.in,cp=cp.in))
y = (as.numeric(zone$description) - 1)[CV.ind.mat[k,]]
p.a = predict(model.a,newdata=test.data,type="prob")[,2]
p.b = predict(model.b,newdata=test.data)$prob[,2]
#Probably don't want rounded probabilities to exactly 0 or 1
p.b[p.b==0] = 0.00001
p.b[p.b==1] = .9999
LL.a1 = sum(y*log(p.a) + (1-y)*log(1-p.a))
LL.b1 = sum(y*log(p.b) + (1-y)*log(1-p.b))
CVLL.a = CVLL.a + LL.a1
CVLL.b = CVLL.b + LL.b1
}
ROW <- c(CVLL.a,CVLL.b,ntree.in, mfinal.in, cp.in, ms.in)
return(ROW)
}
FiveFold(200,200,.01,8)
start = date()
FiveFold(200,200,.01,8)
end = date()
FiveFold <- function(ntree.in, mfinal.in, cp.in, ms.in){
###model b wins by a mile, but I'm a little worried about overfitting.  Time to cross validate)
###Going to 10 fold cross validate
set.seed(536)
n = dim(zone)[1]
###with 179 data points, going to have 17 data points in each testing set
###Obviously that means there are 9 data points that never get partitioned
###as testing data.  But we can pick those up as their own testing group at the end if we want to.
CV.ind.mat = matrix(sample(1:n,3070,replace=F),nrow=5)
#Run same code as before, but each time, we're going to run our models using only train data
#and testing them using only testing data.
CVLL.a = 0
CVLL.b = 0
for(k in 1:5){
train.data = zone[(1:n)[-CV.ind.mat[k,]],]
test.data = zone[CV.ind.mat[k,],]
model.a = randomForest(description ~ plate_x + plate_z + balls + strikes + release_speed + home_score + away_score,
data = train.data, ntree = ntree.in, mtry = 3, control=rpart.control(minsplit=ms.in,cp=cp.in))
model.b = boosting(description ~ plate_x + plate_z + balls + strikes + release_speed + home_score + away_score,
data = train.data, mfinal=mfinal.in, control=rpart.control(minsplit=ms.in,cp=cp.in))
y = (as.numeric(zone$description) - 1)[CV.ind.mat[k,]]
p.a = predict(model.a,newdata=test.data,type="prob")[,2]
p.b = predict(model.b,newdata=test.data)$prob[,2]
#Probably don't want rounded probabilities to exactly 0 or 1
p.a[p.a==0] = 0.00001
p.a[p.a==1] = .9999
p.b[p.b==0] = 0.00001
p.b[p.b==1] = .9999
LL.a1 = sum(y*log(p.a) + (1-y)*log(1-p.a))
LL.b1 = sum(y*log(p.b) + (1-y)*log(1-p.b))
CVLL.a = CVLL.a + LL.a1
CVLL.b = CVLL.b + LL.b1
}
ROW <- c(CVLL.a,CVLL.b,ntree.in, mfinal.in, cp.in, ms.in)
return(ROW)
}
start = date()
FiveFold(200,200,.01,8)
end = date()
LLdf <- data.frame()
start = date()
for(i.tr in c(100,500,1000)){
for(i.cp in c(.001,.01,.05)){
for(i.ms in c(5,10,20)){
row <- FiveFold(i.tr,i.tr,i.cp,i.ms)
LLdf <- rbind(LLdf,row)
}
}
}
View(LLdf)
FiveFold(200,200,.01,8)
min(min(LLdf$X.1875.95867083072),min(LLdf$X.1901.5264633744))
max(max(LLdf$X.1875.95867083072),max(LLdf$X.1901.5264633744))
startF = date()
FiveFold(1000,1000,.05,5) -> x1
# render idealmodel here
idealmodel = boosting(equa1,data=zone,mfinal=500,control=rpart.control(minsplit=10,cp=.01))
# if the idealmodel is AB
P0 = predict(idealmodel,newdata=zone0)$prob[,2]
P1 = predict(idealmodel,newdata=zone1)$prob[,2]
P2 = predict(idealmodel,newdata=zone2)$prob[,2]
P3 = predict(idealmodel,newdata=zone3)$prob[,2]
# Plotting multiple boxplots on the same graph
boxplot(P0, P1, P2, P3,
names = c("Balls = 0", "Balls = 1", "Balls = 2", "Balls = 3"),
col = c("skyblue", "lightgreen", "lightpink", "blanchedalmond"),
main = "Multiple Boxplots",
xlab = "Groups",
ylab = "Probability")
Probdat <- cbind(P0,P1,P2,P3)
?write.csv()
write.csv(Probdat, "probdat.csv")
head(Probdat)
P02 <- c(P0,P1,P2)
P02
mean(P02)
mean(P3)
?T.test()
?t.test()
t.test(P02,P3,
alternative = "two.sided",
conf = .95)
t.test(P02,P3,
alternative = "two.sided",
conf = .99)
t.test(P02,P3,
alternative = "two.sided",
conf = .95)
t.test(P02,P3,
alternative = "two.sided",
conf = .999)
t.test(P02,P3,
alternative = "two.sided",
conf = .99)
t.test(P02,
alternative = "two.sided",
conf = .99)
mean(P3)
t.test(P02,P3,
alternative = "two.sided",
conf = .99)
hist(P0)
hist(P02)
hist(P3)
# Plotting multiple boxplots on the same graph
boxplot(P02, P3,
names = c("Balls = 0,1,2", "Balls = 3"),
col = c("skyblue", "blanchedalmond"),
main = "Multiple Boxplots",
xlab = "Groups",
ylab = "Probability")
# Plotting multiple boxplots on the same graph
boxplot(P02, P3,
names = c("Balls = 0,1,2", "Balls = 3"),
col = c("skyblue", "blanchedalmond"),
main = "Prob. of calling strike",
xlab = "Groups",
ylab = "Probability")
# Plotting multiple boxplots on the same graph
boxplot(P02, P3,
names = c("Balls = 0,1,2", "Balls = 3"),
col = c("skyblue", "blanchedalmond"),
main = "Prob. of Calling Strike",
xlab = "Groups",
ylab = "Probability")
qqplot(P02);qqplot(P3)
# Plotting multiple boxplots on the same graph
boxplot(P02, P3,
names = c("Balls = 0,1,2", "Balls = 3"),
col = c("skyblue", "blanchedalmond"),
main = "Prob. of Calling Strike",
xlab = "Groups",
ylab = "Probability")
qqplot(P02);qqplot(P3)
# Plotting multiple boxplots on the same graph
boxplot(P02, P3,
names = c("Balls = 0,1,2", "Balls = 3"),
col = c("skyblue", "blanchedalmond"),
main = "Prob. of Calling Strike",
xlab = "Groups",
ylab = "Probability")
qqnorm(P02);qqnorm(P3)
# Plotting multiple boxplots on the same graph
boxplot(P02, P3,
names = c("Balls = 0,1,2", "Balls = 3"),
col = c("skyblue", "blanchedalmond"),
main = "Prob. of Calling Strike",
xlab = "Groups",
ylab = "Probability")
qqnorm(P02);qqnorm(P3)
# make a conf interval
quantile(P02,0.005)
quantile(P02,0.005)
# Plotting multiple boxplots on the same graph
boxplot(P02, P3,
names = c("Balls = 0,1,2", "Balls = 3"),
col = c("skyblue", "blanchedalmond"),
main = "Prob. of Calling Strike",
xlab = "Groups",
ylab = "Probability")
qqnorm(P02);qqnorm(P3)
# make a conf interval
quantile(P02,0.005)
quantile(P02,1-0.005)
# Plotting multiple boxplots on the same graph
boxplot(P02, P3,
names = c("Balls = 0,1,2", "Balls = 3"),
col = c("skyblue", "blanchedalmond"),
main = "Prob. of Calling Strike",
xlab = "Groups",
ylab = "Probability")
qqnorm(P02);qqnorm(P3)
# make a conf interval
mean(P02) + (mean(P02) - quantile(P02,0.005))*sqrt(var(P02)/length(P02))
mean(P02) - (mean(P02) - quantile(P02,0.005))*sqrt(var(P02)/length(P02))
# Plotting multiple boxplots on the same graph
boxplot(P02, P3,
names = c("Balls = 0,1,2", "Balls = 3"),
col = c("skyblue", "blanchedalmond"),
main = "Prob. of Calling Strike",
xlab = "Groups",
ylab = "Probability")
qqnorm(P02);qqnorm(P3)
# make a conf interval
mean(P02) + (mean(P02) - quantile(P02,0.005))*sqrt(var(P02)/length(P02))
mean(P02) - (mean(P02) - quantile(P02,1-0.005))*sqrt(var(P02)/length(P02))
# Plotting multiple boxplots on the same graph
boxplot(P02, P3,
names = c("Balls = 0,1,2", "Balls = 3"),
col = c("skyblue", "blanchedalmond"),
main = "Prob. of Calling Strike",
xlab = "Groups",
ylab = "Probability")
qqnorm(P02);qqnorm(P3)
# make a conf interval
mean(P02)
mean(P3)
mean(P02) + quantile(P02,1-0.005)*sqrt(var(P02)/length(P02))
# Plotting multiple boxplots on the same graph
boxplot(P02, P3,
names = c("Balls = 0,1,2", "Balls = 3"),
col = c("skyblue", "blanchedalmond"),
main = "Prob. of Calling Strike",
xlab = "Groups",
ylab = "Probability")
qqnorm(P02);qqnorm(P3)
# make a conf interval
mean(P02)
mean(P3)
# confidence interval
mean(P02) + quantile(P02,0.005)*sqrt(var(P02)/length(P02))
mean(P02) + quantile(P02,1-0.005)*sqrt(var(P02)/length(P02))
# Plotting multiple boxplots on the same graph
boxplot(P02, P3,
names = c("Balls = 0,1,2", "Balls = 3"),
col = c("skyblue", "blanchedalmond"),
main = "Prob. of Calling Strike",
xlab = "Groups",
ylab = "Probability")
qqnorm(P02);qqnorm(P3)
# make a conf interval
mean(P02)
mean(P3)
mean(P02) - mean(P3)
# confidence interval
mean(P02) + quantile(P02,0.005)*sqrt(var(P02)/length(P02))
mean(P02) + quantile(P02,1-0.005)*sqrt(var(P02)/length(P02))
?anova()
BigP <- c(P0,P1,P2,P3)
BigP
P0$ball = 0
as.vector(P0)
P0 =as.vector(P0)
# if the idealmodel is AB
P0 = predict(idealmodel,newdata=zone0)$prob[,2]
PBall <- rep(0,4*3074)
PBall[0:3074] = 0
Pball[(3074+1):(3074+3074)] = 1
PBall[(3074+1):(3074+3074)] = 1
P0 <- as.data.frame(P0)
P0$B <- 0
P0 <- as.data.frame(P0)
P0$B <- 0
P0 <- as.matrix(P0)
P1 <- as.data.frame(P1)
P1$B <- 0
P1 <- as.matrix(P1)
P2 <- as.data.frame(P2)
P2$B <- 0
P2 <- as.matrix(P2)
P3 <- as.data.frame(P3)
P3$B <- 0
P3 <- as.matrix(P3)
BigP <- rbind(P0,P1,P2,P3)
dim(BigP)
View(BigP)
P0 <- as.data.frame(P0)
P0$B <- 0
P0 <- as.matrix(P0)
P1 <- as.data.frame(P1)
P1$B <- 1
P1 <- as.matrix(P1)
P2 <- as.data.frame(P2)
P2$B <- 2
P2 <- as.matrix(P2)
P3 <- as.data.frame(P3)
P3$B <- 3
P3 <- as.matrix(P3)
BigP <- rbind(P0,P1,P2,P3)
unique(BigP$B)
unique(BigP[,2])
aov(BigP)
aov(BigP)
P0 <- as.data.frame(P0)
P0$B <- "x0"
P0 <- as.matrix(P0)
P1 <- as.data.frame(P1)
P1$B <- "x1"
P1 <- as.matrix(P1)
P2 <- as.data.frame(P2)
P2$B <- "x2"
P2 <- as.matrix(P2)
P3 <- as.data.frame(P3)
P3$B <- "x3"
P3 <- as.matrix(P3)
BigP <- rbind(P0,P1,P2,P3)
unique(BigP[,2])
aov(y~x0+x1+x2+x3,data = BigP)
aov(y~x0+x1+x2+x3,data = as.data.frame(BigP))
# if the idealmodel is AB
P0 = predict(idealmodel,newdata=zone0)$prob[,2]
P1 = predict(idealmodel,newdata=zone1)$prob[,2]
P2 = predict(idealmodel,newdata=zone2)$prob[,2]
P3 = predict(idealmodel,newdata=zone3)$prob[,2]
(P0,P1,P2,P3)
BigP <- data.frame(c(P0,P1,P2,P3))
fligner.test(value ~ P0 + P1 + P2 + P3, data = BigP)
BigP <- as.data.frame(cbind(P0,P1,P2,P3))
fligner.test(value ~ P0 + P1 + P2 + P3, data = BigP)
fligner.test(names(BigP) ~ P0 + P1 + P2 + P3, data = BigP)
View(BigP)
names(BigP)
BigPdata <- data.frame(group = c(P0,P1,P2,P3),
value = c(P0,P1,P2,P3))
fligner.test(value ~ group, data = BigPdata)
for(i in 1:4){
mean(BigP[,i])
}
for(i in 1:4){
print(mean(BigP[,i]))
}
for(i in 1:4){
print(mean(BigP[,i]))
print(fivenum(BigP[,i]))
}
names(BigP)[1]
for(i in 1:4){
names(BigP[i])
print(mean(BigP[,i]))
print(fivenum(BigP[,i]))
}
for(i in 1:4){
print(names(BigP[i]))
print(mean(BigP[,i]))
print(fivenum(BigP[,i]))
}
#BigP <- as.data.frame(cbind(P0,P1,P2,P3))
for(i in 1:4){
print(names(BigP[i]))
print(mean(BigP[,i]))
print(fivenum(BigP[,i]))
}
#BigP <- as.data.frame(cbind(P0,P1,P2,P3))
for(i in 1:4){
print(names(BigP[i]))
print(mean(BigP[,i]))
print(fivenum(BigP[,i]))
}
View(zone)
# Plotting multiple boxplots on the same graph
boxplot(P0, P1, P2, P3,
names = c("Balls = 0", "Balls = 1", "Balls = 2", "Balls = 3"),
col = c("skyblue", "lightgreen", "lightpink", "blanchedalmond"),
main = "Probability of Umpire Calling Strike",
xlab = "Groups",
ylab = "Probability")
knitr::opts_chunk$set(echo = TRUE)
data <- read.table("TotalSales-2.txt",header = T)
series <- ts(data,frequency = 4)
library(dplyr)
# diagnostics for fit 1
par(mfrow=c(2,2)) # Dividing the plotting page into 4 panels
plot(fit2)
knitr::opts_chunk$set(echo = TRUE)
data <- read.table("TotalSales-2.txt",header = T)
series <- ts(data,frequency = 4)
library(dplyr)
set.seed(536.1)
# initialize X
sample(c(-1,1),size = 500,prob = c(.5,.5),replace = T) -> X
# initialize Svec
Svec = cumsum(c(0,X))
# make graphics
plot.ts(Svec,type = "l",ylab = "S_t", xlab = "t")
acf(Svec)
# running means
Svec_bar = Svec
for(i in 1:501){
mean(Svec[1:i]) -> Svec_bar[i]
}
# plot running means
plot(Svec_bar,type = 'l')
plot.ts(series,type = 'l',ylab = "series")
abline(lm(series ~ time(series),data = data),lty = 2)
plot.ts(log(series),type = "l",ylab="log of series")
abline(lm(log(series) ~ time(series),data = data), lty =2)
#define a t and cycle
t = time(series)
cyc = as.factor(cycle(series))
# test two regressions
fit1 <- lm(log(series) ~ cyc + t)
fit2 = lm(log(series) ~ (cyc*t)^3 + cyc*t+ cyc)
plot.ts(log(series),type = "l",ylab="log of series")
points(t,predict.lm(fit1),type = "l", col = "red" )
points(t,predict.lm(fit2),type = "l", col = "blue" )
# diagnostics for fit 1
par(mfrow=c(2,2)) # Dividing the plotting page into 4 panels
plot(fit1)
# diagnostics for fit 1
par(mfrow=c(2,2)) # Dividing the plotting page into 4 panels
plot(fit1)
acf(fit1$residuals) #sample acf plot of residuals
# diagnostics for fit 1
par(mfrow=c(2,2)) # Dividing the plotting page into 4 panels
plot(fit2)
acf(fit2$residuals) #sample acf plot of residuals
M = matrix(c(1,2,3,4,5,6,7,8,9),nrow = 3, ncol = 3, byrow = T)
M
setwd("~/Desktop/githubbahubba/CSUF/summer 2024/Exam3")
knitr::opts_chunk$set(echo = TRUE)
library(tseries)
library(forecast)
phi = 0.6
the = 0.9
SIM  <- as.matrix(NULL)
SIM  <- as.matrix(0,100,3)
SIM[,1] <- arima.sim(n=100, list(ar = phi, ma = the))
SIM  <- matrix(0,100,3)
SIM[,1] <- arima.sim(n=100, list(ar = phi, ma = the))
SIM[,2] <- arima.sim(n=100, list(ar = phi))
SIM[,3] <- arima.sim(n=100, list(ma = the))
SIM[,1] <- arima.sim(n=100, list(ar = phi, ma = the))
SIM[,2] <- arima.sim(n=100, list(ar = phi))
SIM[,3] <- arima.sim(n=100, list(ma = the))
View(SIM)
# plot the timeseries
for(i in 1:3){plot.ts(SIM[,i])}
for(i in 1:3){plot.ts(SIM[,i],main = name[i])}
for(i in 1:3){plot.ts(SIM[,i],title = name[i])}
# plot the timeseries
name = c("ARMA(1,1)","ARMA(1,0)","ARMA(0,1)")
for(i in 1:3){plot.ts(SIM[,i],main = "")}
for(i in 1:3){plot.ts(SIM[,i],main = "p")}
# plot the timeseries
name = c("ARMA(1,1)","ARMA(1,0)","ARMA(0,1)")
for(i in 1:3){plot.ts(SIM[,i],main = name[i])}
"hollow" + name -> name1
pacf(SIM[,1])
for(i in 1:3){plot.ts(SIM[,i],main = name[i]), ylab = "X"}
# plot the timeseries
name = c("ARMA(1,1)","ARMA(1,0)","ARMA(0,1)")
for(i in 1:3){plot.ts(SIM[,i],main = name[i]), ylab = "X"}
# plot the timeseries
name = c("ARMA(1,1)","ARMA(1,0)","ARMA(0,1)")
for(i in 1:3){plot.ts(SIM[,i],main = name[i], ylab = "X")}
# plot the ACFs
for(i in 1:3){acf(SIM[,i],main = name[i],ylab = "ACF")}
# plot the PACFs
for(i in 1:3){pacf(SIM[,i],main = name[i],ylab = "PACF")}
# plot the PACFs
for(i in 1:3){pacf(SIM[,i],main = name[i],ylab = "alpha(h)")}
# plot the ACFs
for(i in 1:3){acf(SIM[,i],main = name[i],ylab = "rho(h)")}
