---
title: "Exam 3"
author: "Michael Pena"
date: "2024-06-13"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tseries)
library(forecast)
```

## Question 1

We found previously that
$$
\nabla_4X_t = 4\beta_1 + Z_t + Z_{t-4} 
$$
for the seasonal differencing, we find that 

$$
\nabla_4 \nabla X_t = -Z_{t-1} -Z_{t-4} + Z_{t-5}
$$
One has more parameters than the other. essentially in the end we want to go with the model that works with less parameters and does the same job in the end; less parameters means less work for the same result (we would pick the seasonal differenced pathway).

## Question 2

```{r}
# define the coeffs
phi = 0.6
the = 0.9
# save simulations into a matrix
SIM  <- matrix(0,100,3)
SIM[,1] <- arima.sim(n=100, list(ar = phi, ma = the))
SIM[,2] <- arima.sim(n=100, list(ar = phi))
SIM[,3] <- arima.sim(n=100, list(ma = the))
```

```{r}
# plot the timeseries
name = c("ARMA(1,1)","ARMA(1,0)","ARMA(0,1)")
for(i in 1:3){plot.ts(SIM[,i],main = name[i], ylab = "X_t")}
```

```{r}
# plot the ACFs
for(i in 1:3){acf(SIM[,i],main = name[i],ylab = "rho(h)")}
```

```{r}
# plot the PACFs
for(i in 1:3){pacf(SIM[,i],main = name[i],ylab = "alpha(h)")}
```

The models seem to match the table we went over in class; I do think that even though there seems to be a significant peak in the ARIMA(1,0) PACF, it seems not to peak over enough to claim to be an ARIMA(2,0) model.

## Question 3

```{r}
# get the data
library(astsa)
# make models
ar.ols(cmort,order= 2) -> cmort.ar2
arima(cmort, order = c(2,0,0)) -> cmort.arima200
cmort.ar2$x.intercept
summary(cmort.arima200)
```

```{r}
ts.plot(cmort, main = "fitting with ar.ols()", ylab = "Mortality")
lines(fitted(cmort.ar2),col = 'red')

```
```{r}
ts.plot(cmort, main = "fitting with arima()", ylab = "Mortality")
lines(fitted(cmort.arima200),col = 'skyblue')
```

## Question 4

### part (a)

```{r}
# getting data
demand <- as.vector(read.csv("Demand-2.txt", head = T)[,1])
series <- ts(demand, start =c(1992,1), frequency = 12)
series.train <- ts(demand[1:263], start =c(1992,1), frequency = 12)
series.val <- ts(demand[264:287], start =c(2014,1), frequency = 12)

par(mfrow=c(1,2))
plot.ts(series.train)
plot.ts(log(series.train))
acf(series.train)
acf(log(series.train))
boxplot(series.train)
boxplot(log(series.train))
```

Going to try more transforms

```{r}
plot.ts(sqrt(series.train))
plot.ts((series.train)^(1/3))
```


```{r}
# fit linear model :|
t = time(series.train)
fit0 <- lm(series.train ~ t)
fit1 <- lm(series.train ~ t + t^2)
fit2 <- lm(series.train ~ t + t^2 + t^3)
fit3 <- lm(series.train ~ t + t^2 + t^3 + t^4)
fit4 <- lm(sqrt(series.train) ~ t)
fit5 <- lm((series.train)^(1/3) ~ t)

par(mfrow = c(2,2))
plot(fit0)
plot(fit1)
plot(fit2)
plot(fit3)
plot(fit4)
plot(fit5)
```

the scale-location line doesn't seem to straighten out for any linear fit we do. I am going to difference.

```{r}
#difference once 

series.trainD <- diff(series.train,diff = 1)
# run kpss test
series.trainD_decomp <- decompose(series.trainD,type = c("additive"))
series.trainD_decomp <- na.omit(series.trainD_decomp)
kpss.test(series.trainD_decomp$random)
```

The test concludes a failure to reject null, there is a chance that the system is stationary, so lets check variance again.

```{r}
par(mfrow=c(2,2))
plot(lm(series.trainD ~ time(series.trainD)))
```

Yay. It is stabalized now!

### part (b)

```{r}
# render graphs the single out random cycles of 12
par(mfrow=c(2,2))
plot.ts(series.trainD[25:36])
plot.ts(series.trainD[145:156])
plot.ts(series.trainD[217:228])
```
It seems in the later months, there are peaks at the 2nd, 6th, and 10th month while there are dips in the 4th, 7th, 9th and 12th month. This is not consistent in the earlier months.

```{r}
# decomposing the series.train (we have done this previously in (a))
plot(series.trainD_decomp) 
```

I feel it necessary to try a moving average smoother to find a trend less erratic.

```{r}
# trying kernal smoothing from the book pp.74
plot(series.trainD, main= "Kernal Smoother Overlay")
lines(ksmooth(time(series.trainD), series.trainD, "normal", bandwidth=1), lwd=2, col=4)
par(fig = c(.65, 1, .65, 1), new = TRUE) # the insert
gauss = function(x) { 1/sqrt(2*pi) * exp(-(x^2)/2) }
```
This looks close to what we got from the decomposed series before. Going to try a smoothing spline for this task.

```{r}
plot(series.trainD, main= "Kernal Smoothing Spline Overlay")
lines(smooth.spline(time(series.trainD), series.trainD, spar=.3), lwd=2, col=4)
```

Let's see how this compares to differencing a decomposing one more time.

```{r}
series.trainD2 <- diff(series.train,diff = 2)
series.trainD2_decomp <- decompose(series.trainD2,type = c("additive"))
series.trainD2_decomp <- na.omit(series.trainD2_decomp)
plot(series.trainD2_decomp) 
```
That made the general trend more spiked; let's stick with the MA kernal smoother.

```{r}
yspline.hat <- ts(smooth.spline(time(series.trainD), series.trainD, spar=.3)$y, start =c(1992,1), frequency = 12)
yspline.hat_decomp <- decompose(yspline.hat,type = c("additive"))
yspline.hat_decomp <- na.omit(yspline.hat_decomp)
plot(yspline.hat_decomp$random, main = "Random component")
plot(yspline.hat_decomp$seasonal, main = "Random component") 

```

```{r}
# testing both for stationarity
# residuals
yspline.hat_decompR <- na.omit(yspline.hat_decomp$random)
kpss.test(yspline.hat_decompR)
# seasonal
yspline.hat_decompS <- na.omit(yspline.hat_decomp$seasonal)
kpss.test(yspline.hat_decompS)
```

Both test statistics fall in the the "Fail To Reject" region; both systems are stationary.

### part (c)


```{r}
par(mfrow = c(1,2))
acf(series.train)
pacf(series.train)
acf(series.trainD)
pacf(series.trainD)
```

I believe it fair to suggest that p = 0 and q = 1; there is are no immediate spikes in PACF graph while there is a single one at the beginning of the ACF graph. 

We saw previously, after differencing once, seasonality every 12 months (s = 12). This makes d = 1 but  we notice a spike in both graphs at every whole number (note the view of the graph cuts of after 2.0). This can imply P $\geq$ 2 and Q $\geq$ 2. I did not conduct seasonal differencing thus D = 0. 

### part (d)

The view of the ACF and PACF graphs cuts off after 2.0. There could be more spikes at later lags.

I will test the following ARIMAs
(0,1,1)$\times$(2,0,2)$_{12}$
(0,1,1)$\times$(3,0,3)$_{12}$
(0,1,1)$\times$(4,0,4)$_{12}$